From: Linux Kernel Mailing List <linux-kernel@vger.kernel.org>
Subject: Linux: 3.5
Patch-mainline: 3.5

 This patch contains the differences between 3.4 and 3.5.

Automatically created from "patch-3.5" by xen-port-patches.py
Acked-by: jbeulich@suse.com

--- head.orig/arch/x86/Kbuild	2012-06-13 15:13:30.000000000 +0200
+++ head/arch/x86/Kbuild	2012-06-14 12:03:04.000000000 +0200
@@ -6,7 +6,9 @@ obj-$(CONFIG_PARAVIRT_XEN) += xen/
 # lguest paravirtualization support
 obj-$(CONFIG_LGUEST_GUEST) += lguest/
 
+ifneq ($(CONFIG_XEN),y)
 obj-y += realmode/
+endif
 obj-y += kernel/
 obj-y += mm/
 
--- head.orig/arch/x86/Kconfig	2012-08-20 13:18:38.000000000 +0200
+++ head/arch/x86/Kconfig	2012-06-14 11:59:13.000000000 +0200
@@ -32,7 +32,7 @@ config X86
 	select ARCH_WANT_OPTIONAL_GPIOLIB
 	select ARCH_WANT_FRAME_POINTERS
 	select HAVE_DMA_ATTRS
-	select HAVE_DMA_CONTIGUOUS if !SWIOTLB
+	select HAVE_DMA_CONTIGUOUS if !SWIOTLB && !XEN
 	select HAVE_KRETPROBES
 	select HAVE_OPTPROBES
 	select HAVE_FTRACE_MCOUNT_RECORD
--- head.orig/arch/x86/ia32/ia32entry-xen.S	2012-02-09 12:46:23.000000000 +0100
+++ head/arch/x86/ia32/ia32entry-xen.S	2012-06-14 11:23:26.000000000 +0200
@@ -13,6 +13,7 @@
 #include <asm/thread_info.h>	
 #include <asm/segment.h>
 #include <asm/irqflags.h>
+#include <asm/asm.h>
 #include <linux/linkage.h>
 #include <linux/err.h>
 
@@ -138,9 +139,7 @@ ENTRY(ia32_sysenter_target)
  	/* no need to do an access_ok check here because rbp has been
  	   32bit zero extended */ 
 1:	movl	(%rbp),%ebp
- 	.section __ex_table,"a"
- 	.quad 1b,ia32_badarg
- 	.previous	
+	_ASM_EXTABLE(1b,ia32_badarg)
 	orl     $TS_COMPAT,TI_status+THREAD_INFO(%rsp,RIP-ARGOFFSET)
 	testl   $_TIF_WORK_SYSCALL_ENTRY,TI_flags+THREAD_INFO(%rsp,RIP-ARGOFFSET)
 	jnz  sysenter_tracesys
@@ -214,9 +213,7 @@ ENTRY(ia32_cstar_target)
 	   32bit zero extended */ 
 	/* hardware stack frame is complete now */	
 1:	movl	(%r8),%r9d
-	.section __ex_table,"a"
-	.quad 1b,ia32_badarg
-	.previous	
+	_ASM_EXTABLE(1b,ia32_badarg)
 	orl     $TS_COMPAT,TI_status+THREAD_INFO(%rsp,RIP-ARGOFFSET)
 	testl   $_TIF_WORK_SYSCALL_ENTRY,TI_flags+THREAD_INFO(%rsp,RIP-ARGOFFSET)
 	jnz   cstar_tracesys
--- head.orig/arch/x86/include/asm/io_apic.h	2012-07-21 22:58:29.000000000 +0200
+++ head/arch/x86/include/asm/io_apic.h	2012-06-14 12:07:45.000000000 +0200
@@ -175,6 +175,7 @@ extern void mp_save_irq(struct mpc_intsr
 
 extern void disable_ioapic_support(void);
 
+#ifndef CONFIG_XEN
 extern void __init native_io_apic_init_mappings(void);
 extern unsigned int native_io_apic_read(unsigned int apic, unsigned int reg);
 extern void native_io_apic_write(unsigned int apic, unsigned int reg, unsigned int val);
@@ -193,6 +194,7 @@ static inline void io_apic_modify(unsign
 {
 	x86_io_apic_ops.modify(apic, reg, value);
 }
+#endif /* !CONFIG_XEN */
 #else  /* !CONFIG_X86_IO_APIC */
 
 #define io_apic_assign_pci_irqs 0
--- head.orig/arch/x86/include/mach-xen/asm/desc.h	2012-02-09 12:32:50.000000000 +0100
+++ head/arch/x86/include/mach-xen/asm/desc.h	2012-06-14 11:23:26.000000000 +0200
@@ -6,6 +6,7 @@
 #include <asm/mmu.h>
 
 #include <linux/smp.h>
+#include <linux/percpu.h>
 
 static inline void fill_ldt(struct desc_struct *desc, const struct user_desc *info)
 {
--- head.orig/arch/x86/include/mach-xen/asm/dma-mapping.h	2012-04-11 14:25:16.000000000 +0200
+++ head/arch/x86/include/mach-xen/asm/dma-mapping.h	2012-06-14 11:23:26.000000000 +0200
@@ -18,9 +18,6 @@ static inline phys_addr_t dma_to_phys(st
 	return machine_to_phys(daddr);
 }
 
-void dma_generic_free_coherent(struct device *, size_t, void *, dma_addr_t,
-			       struct dma_attrs *);
-
 extern int range_straddles_page_boundary(paddr_t p, size_t size);
 
 #endif /* _ASM_X86_DMA_MAPPING_H_ */
--- head.orig/arch/x86/include/mach-xen/asm/hypervisor.h	2012-05-11 16:45:57.000000000 +0200
+++ head/arch/x86/include/mach-xen/asm/hypervisor.h	2012-06-14 12:25:31.000000000 +0200
@@ -156,16 +156,16 @@ int __must_check xen_multi_mmuext_op(str
 #define __HAVE_ARCH_ENTER_LAZY_MMU_MODE
 static inline void arch_enter_lazy_mmu_mode(void)
 {
-	percpu_write(xen_lazy_mmu, true);
+	this_cpu_write_1(xen_lazy_mmu, true);
 }
 
 static inline void arch_leave_lazy_mmu_mode(void)
 {
-	percpu_write(xen_lazy_mmu, false);
+	this_cpu_write_1(xen_lazy_mmu, false);
 	xen_multicall_flush();
 }
 
-#define arch_use_lazy_mmu_mode() unlikely(percpu_read(xen_lazy_mmu))
+#define arch_use_lazy_mmu_mode() unlikely(this_cpu_read_1(xen_lazy_mmu))
 
 #if 0 /* All uses are in places potentially called asynchronously, but
        * asynchronous code should rather not make use of lazy mode at all.
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ head/arch/x86/include/mach-xen/asm/kbdleds.h	2012-06-15 14:06:50.000000000 +0200
@@ -0,0 +1,16 @@
+#ifndef _ASM_X86_KBDLEDS_H
+#define _ASM_X86_KBDLEDS_H
+
+/*
+ * Some laptops take the 789uiojklm,. keys as number pad when NumLock is on.
+ * This seems a good reason to start with NumLock off. That's why on X86 we
+ * ask the bios for the correct state.
+ */
+
+#ifdef CONFIG_XEN_PRIVILEGED_GUEST
+int kbd_defleds(void);
+#else
+static inline int kbd_defleds(void) { return 0; }
+#endif
+
+#endif /* _ASM_X86_KBDLEDS_H */
--- head.orig/arch/x86/include/mach-xen/asm/mmu_context.h	2011-09-08 16:54:08.000000000 +0200
+++ head/arch/x86/include/mach-xen/asm/mmu_context.h	2012-06-14 11:23:26.000000000 +0200
@@ -30,8 +30,8 @@ void destroy_context(struct mm_struct *m
 static inline void enter_lazy_tlb(struct mm_struct *mm, struct task_struct *tsk)
 {
 #if defined(CONFIG_SMP) && !defined(CONFIG_XEN) /* XEN: no lazy tlb */
-	if (percpu_read(cpu_tlbstate.state) == TLBSTATE_OK)
-		percpu_write(cpu_tlbstate.state, TLBSTATE_LAZY);
+	if (this_cpu_read(cpu_tlbstate.state) == TLBSTATE_OK)
+		this_cpu_write(cpu_tlbstate.state, TLBSTATE_LAZY);
 #endif
 }
 
@@ -88,8 +88,8 @@ static inline void switch_mm(struct mm_s
 		       !PagePinned(virt_to_page(next->pgd)));
 
 #if defined(CONFIG_SMP) && !defined(CONFIG_XEN) /* XEN: no lazy tlb */
-		percpu_write(cpu_tlbstate.state, TLBSTATE_OK);
-		percpu_write(cpu_tlbstate.active_mm, next);
+		this_cpu_write(cpu_tlbstate.state, TLBSTATE_OK);
+		this_cpu_write(cpu_tlbstate.active_mm, next);
 #endif
 		cpumask_set_cpu(cpu, mm_cpumask(next));
 
@@ -123,8 +123,8 @@ static inline void switch_mm(struct mm_s
 	}
 #if defined(CONFIG_SMP) && !defined(CONFIG_XEN) /* XEN: no lazy tlb */
 	else {
-		percpu_write(cpu_tlbstate.state, TLBSTATE_OK);
-		BUG_ON(percpu_read(cpu_tlbstate.active_mm) != next);
+		this_cpu_write(cpu_tlbstate.state, TLBSTATE_OK);
+		BUG_ON(this_cpu_read(cpu_tlbstate.active_mm) != next);
 
 		if (!cpumask_test_and_set_cpu(cpu, mm_cpumask(next))) {
 			/* We were in lazy tlb mode and leave_mm disabled
--- head.orig/arch/x86/include/mach-xen/asm/percpu.h	2011-12-23 10:46:09.000000000 +0100
+++ head/arch/x86/include/mach-xen/asm/percpu.h	2012-07-05 12:31:24.000000000 +0200
@@ -3,6 +3,14 @@
 
 #include_next <asm/percpu.h>
 
+#ifdef CONFIG_64BIT
+# define __this_cpu_read_l  __this_cpu_read_8
+# define __this_cpu_write_l __this_cpu_write_8
+#else
+# define __this_cpu_read_l  __this_cpu_read_4
+# define __this_cpu_write_l __this_cpu_write_4
+#endif
+
 #define this_vcpu_read_1 this_cpu_read_1
 #define this_vcpu_read_2 this_cpu_read_2
 #define this_vcpu_read_4 this_cpu_read_4
--- head.orig/arch/x86/include/mach-xen/asm/pgtable.h	2012-02-09 12:32:50.000000000 +0100
+++ head/arch/x86/include/mach-xen/asm/pgtable.h	2012-06-18 09:47:52.000000000 +0200
@@ -604,6 +604,7 @@ static inline int pgd_none(pgd_t pgd)
 #define KERNEL_PGD_PTRS		(PTRS_PER_PGD - KERNEL_PGD_BOUNDARY)
 
 #ifndef __ASSEMBLY__
+#include <asm/tlbflush.h>
 
 #define direct_gbpages 0
 
--- head.orig/arch/x86/include/mach-xen/asm/pgtable-3level.h	2011-04-12 15:59:10.000000000 +0200
+++ head/arch/x86/include/mach-xen/asm/pgtable-3level.h	2012-07-05 12:45:30.000000000 +0200
@@ -34,6 +34,60 @@ static inline void xen_set_pte(pte_t *pt
 	ptep->pte_low = pte.pte_low;
 }
 
+#define pmd_read_atomic pmd_read_atomic
+/*
+ * pte_offset_map_lock on 32bit PAE kernels was reading the pmd_t with
+ * a "*pmdp" dereference done by gcc. Problem is, in certain places
+ * where pte_offset_map_lock is called, concurrent page faults are
+ * allowed, if the mmap_sem is hold for reading. An example is mincore
+ * vs page faults vs MADV_DONTNEED. On the page fault side
+ * pmd_populate rightfully does a set_64bit, but if we're reading the
+ * pmd_t with a "*pmdp" on the mincore side, a SMP race can happen
+ * because gcc will not read the 64bit of the pmd atomically. To fix
+ * this all places running pmd_offset_map_lock() while holding the
+ * mmap_sem in read mode, shall read the pmdp pointer using this
+ * function to know if the pmd is null nor not, and in turn to know if
+ * they can run pmd_offset_map_lock or pmd_trans_huge or other pmd
+ * operations.
+ *
+ * Without THP if the mmap_sem is hold for reading, the pmd can only
+ * transition from null to not null while pmd_read_atomic runs. So
+ * we can always return atomic pmd values with this function.
+ *
+ * With THP if the mmap_sem is hold for reading, the pmd can become
+ * trans_huge or none or point to a pte (and in turn become "stable")
+ * at any time under pmd_read_atomic. We could read it really
+ * atomically here with a atomic64_read for the THP enabled case (and
+ * it would be a whole lot simpler), but to avoid using cmpxchg8b we
+ * only return an atomic pmdval if the low part of the pmdval is later
+ * found stable (i.e. pointing to a pte). And we're returning a none
+ * pmdval if the low part of the pmd is none. In some cases the high
+ * and low part of the pmdval returned may not be consistent if THP is
+ * enabled (the low part may point to previously mapped hugepage,
+ * while the high part may point to a more recently mapped hugepage),
+ * but pmd_none_or_trans_huge_or_clear_bad() only needs the low part
+ * of the pmd to be read atomically to decide if the pmd is unstable
+ * or not, with the only exception of when the low part of the pmd is
+ * zero in which case we return a none pmd.
+ */
+static inline pmd_t pmd_read_atomic(pmd_t *pmdp)
+{
+	pmdval_t ret;
+	u32 *tmp = (u32 *)pmdp;
+
+	ret = (pmdval_t) (*tmp);
+	if (ret) {
+		/*
+		 * If the low part is null, we must not read the high part
+		 * or we can end up with a partial pmd.
+		 */
+		smp_rmb();
+		ret |= ((pmdval_t)*(tmp + 1)) << 32;
+	}
+
+	return (pmd_t) { ret };
+}
+
 static inline void xen_set_pmd(pmd_t *pmdp, pmd_t pmd)
 {
 	xen_l2_entry_update(pmdp, pmd);
--- head.orig/arch/x86/include/mach-xen/asm/processor.h	2012-05-23 13:41:10.000000000 +0200
+++ head/arch/x86/include/mach-xen/asm/processor.h	2012-06-14 11:23:26.000000000 +0200
@@ -545,13 +545,16 @@ native_load_sp0(struct tss_struct *tss, 
  * enable), so that any CPU's that boot up
  * after us can get the correct flags.
  */
-extern unsigned long		mmu_cr4_features;
+extern unsigned long mmu_cr4_features;
+#define trampoline_cr4_features ((u32 *)NULL)
 
 static inline void set_in_cr4(unsigned long mask)
 {
 	unsigned long cr4;
 
 	mmu_cr4_features |= mask;
+	if (trampoline_cr4_features)
+		*trampoline_cr4_features = mmu_cr4_features;
 	cr4 = read_cr4();
 	cr4 |= mask;
 	write_cr4(cr4);
@@ -562,6 +565,8 @@ static inline void clear_in_cr4(unsigned
 	unsigned long cr4;
 
 	mmu_cr4_features &= ~mask;
+	if (trampoline_cr4_features)
+		*trampoline_cr4_features = mmu_cr4_features;
 	cr4 = read_cr4();
 	cr4 &= ~mask;
 	write_cr4(cr4);
@@ -580,9 +585,6 @@ extern int kernel_thread(int (*fn)(void 
 /* Free all resources held by a thread. */
 extern void release_thread(struct task_struct *);
 
-/* Prepare to copy thread state - unlazy all lazy state */
-extern void prepare_to_copy(struct task_struct *tsk);
-
 unsigned long get_wchan(struct task_struct *p);
 
 /*
@@ -975,8 +977,6 @@ extern bool cpu_has_amd_erratum(const in
 #define cpu_has_amd_erratum(x)	(false)
 #endif /* CONFIG_CPU_SUP_AMD */
 
-void cpu_idle_wait(void);
-
 extern unsigned long arch_align_stack(unsigned long sp);
 extern void free_init_pages(char *what, unsigned long begin, unsigned long end);
 
--- head.orig/arch/x86/include/mach-xen/asm/smp.h	2012-02-09 12:32:50.000000000 +0100
+++ head/arch/x86/include/mach-xen/asm/smp.h	2012-06-14 11:23:26.000000000 +0200
@@ -69,6 +69,8 @@ DECLARE_EARLY_PER_CPU(int, x86_cpu_to_lo
 /* Static state in head.S used to set up a CPU */
 extern unsigned long stack_start; /* Initial stack pointer address */
 
+struct task_struct;
+
 struct smp_ops {
 	void (*smp_prepare_boot_cpu)(void);
 	void (*smp_prepare_cpus)(unsigned max_cpus);
@@ -77,7 +79,7 @@ struct smp_ops {
 	void (*stop_other_cpus)(int wait);
 	void (*smp_send_reschedule)(int cpu);
 
-	int (*cpu_up)(unsigned cpu);
+	int (*cpu_up)(unsigned cpu, struct task_struct *tidle);
 	int (*cpu_disable)(void);
 	void (*cpu_die)(unsigned int cpu);
 	void (*play_dead)(void);
@@ -116,9 +118,9 @@ static inline void smp_cpus_done(unsigne
 	smp_ops.smp_cpus_done(max_cpus);
 }
 
-static inline int __cpu_up(unsigned int cpu)
+static inline int __cpu_up(unsigned int cpu, struct task_struct *tidle)
 {
-	return smp_ops.cpu_up(cpu);
+	return smp_ops.cpu_up(cpu, tidle);
 }
 
 static inline int __cpu_disable(void)
@@ -155,13 +157,14 @@ void cpu_disable_common(void);
 void native_smp_prepare_boot_cpu(void);
 void native_smp_prepare_cpus(unsigned int max_cpus);
 void native_smp_cpus_done(unsigned int max_cpus);
-int native_cpu_up(unsigned int cpunum);
+int native_cpu_up(unsigned int cpunum, struct task_struct *tidle);
 int native_cpu_disable(void);
 void native_cpu_die(unsigned int cpu);
 void native_play_dead(void);
 void play_dead_common(void);
 void wbinvd_on_cpu(int cpu);
 int wbinvd_on_all_cpus(void);
+void x86_idle_thread_init(unsigned int cpu, struct task_struct *idle);
 
 void smp_store_cpu_info(int id);
 #define cpu_physical_id(cpu)	per_cpu(x86_cpu_to_apicid, cpu)
--- head.orig/arch/x86/include/mach-xen/asm/smp-processor-id.h	2011-02-01 14:54:13.000000000 +0100
+++ head/arch/x86/include/mach-xen/asm/smp-processor-id.h	2012-06-15 09:34:12.000000000 +0200
@@ -12,7 +12,7 @@ DECLARE_PER_CPU(int, cpu_number);
  * from the initial startup. We map APIC_BASE very early in page_setup(),
  * so this is correct in the x86 case.
  */
-#define raw_smp_processor_id() percpu_read(cpu_number)
+#define raw_smp_processor_id() this_cpu_read_4(cpu_number)
 #define safe_smp_processor_id() smp_processor_id()
 
 #ifdef CONFIG_X86_64_SMP
--- head.orig/arch/x86/include/mach-xen/asm/special_insns.h	2012-07-05 12:19:22.000000000 +0200
+++ head/arch/x86/include/mach-xen/asm/special_insns.h	2012-07-05 12:31:42.000000000 +0200
@@ -13,7 +13,7 @@ DECLARE_PER_CPU(unsigned long, xen_x86_c
 
 static inline unsigned long xen_read_cr0_upd(void)
 {
-	unsigned long upd = percpu_read(xen_x86_cr0_upd);
+	unsigned long upd = __this_cpu_read_l(xen_x86_cr0_upd);
 	rmb();
 	return upd;
 }
@@ -21,17 +21,17 @@ static inline unsigned long xen_read_cr0
 static inline void xen_clear_cr0_upd(void)
 {
 	wmb();
-	percpu_write(xen_x86_cr0_upd, 0);
+	__this_cpu_write_l(xen_x86_cr0_upd, 0);
 }
 
 static inline void xen_clts(void)
 {
 	if (unlikely(xen_read_cr0_upd()))
 		HYPERVISOR_fpu_taskswitch(0);
-	else if (percpu_read(xen_x86_cr0) & X86_CR0_TS) {
-		percpu_write(xen_x86_cr0_upd, X86_CR0_TS);
+	else if (__this_cpu_read_4(xen_x86_cr0) & X86_CR0_TS) {
+		__this_cpu_write_4(xen_x86_cr0_upd, X86_CR0_TS);
 		HYPERVISOR_fpu_taskswitch(0);
-		percpu_and(xen_x86_cr0, ~X86_CR0_TS);
+		__this_cpu_and_4(xen_x86_cr0, ~X86_CR0_TS);
 		xen_clear_cr0_upd();
 	}
 }
@@ -40,10 +40,10 @@ static inline void xen_stts(void)
 {
 	if (unlikely(xen_read_cr0_upd()))
 		HYPERVISOR_fpu_taskswitch(1);
-	else if (!(percpu_read(xen_x86_cr0) & X86_CR0_TS)) {
-		percpu_write(xen_x86_cr0_upd, X86_CR0_TS);
+	else if (!(__this_cpu_read_4(xen_x86_cr0) & X86_CR0_TS)) {
+		__this_cpu_write_4(xen_x86_cr0_upd, X86_CR0_TS);
 		HYPERVISOR_fpu_taskswitch(1);
-		percpu_or(xen_x86_cr0, X86_CR0_TS);
+		__this_cpu_or_4(xen_x86_cr0, X86_CR0_TS);
 		xen_clear_cr0_upd();
 	}
 }
@@ -67,7 +67,7 @@ static inline unsigned long native_read_
 static inline unsigned long xen_read_cr0(void)
 {
 	return likely(!xen_read_cr0_upd()) ?
-	       percpu_read(xen_x86_cr0) : native_read_cr0();
+	       __this_cpu_read_l(xen_x86_cr0) : native_read_cr0();
 }
 
 static inline void native_write_cr0(unsigned long val)
@@ -77,9 +77,9 @@ static inline void native_write_cr0(unsi
 
 static inline void xen_write_cr0(unsigned long val)
 {
-	unsigned long upd = val ^ percpu_read(xen_x86_cr0);
+	unsigned long upd = val ^ __this_cpu_read_l(xen_x86_cr0);
 
-	if (unlikely(percpu_cmpxchg(xen_x86_cr0_upd, 0, upd))) {
+	if (unlikely(percpu_cmpxchg_op(xen_x86_cr0_upd, 0, upd))) {
 		native_write_cr0(val);
 		return;
 	}
@@ -93,7 +93,7 @@ static inline void xen_write_cr0(unsigne
 		native_write_cr0(val);
 		break;
 	}
-	percpu_write(xen_x86_cr0, val);
+	__this_cpu_write_l(xen_x86_cr0, val);
 	xen_clear_cr0_upd();
 }
 
--- head.orig/arch/x86/include/mach-xen/asm/spinlock.h	2012-02-09 12:49:39.000000000 +0100
+++ head/arch/x86/include/mach-xen/asm/spinlock.h	2012-06-14 11:23:26.000000000 +0200
@@ -20,10 +20,8 @@
 
 #ifdef CONFIG_X86_32
 # define LOCK_PTR_REG "a"
-# define REG_PTR_MODE "k"
 #else
 # define LOCK_PTR_REG "D"
-# define REG_PTR_MODE "q"
 #endif
 
 #if defined(CONFIG_XEN) || (defined(CONFIG_X86_32) && \
--- head.orig/arch/x86/include/mach-xen/asm/tlbflush.h	2012-04-11 13:26:23.000000000 +0200
+++ head/arch/x86/include/mach-xen/asm/tlbflush.h	2012-06-14 11:23:26.000000000 +0200
@@ -13,11 +13,7 @@
 #define __flush_tlb_all() xen_tlb_flush()
 #define __flush_tlb_one(addr) xen_invlpg(addr)
 
-#ifdef CONFIG_X86_32
-# define TLB_FLUSH_ALL	0xffffffff
-#else
-# define TLB_FLUSH_ALL	-1ULL
-#endif
+#define TLB_FLUSH_ALL	-1UL
 
 /*
  * TLB flushing:
@@ -98,8 +94,8 @@ DECLARE_PER_CPU_SHARED_ALIGNED(struct tl
 
 static inline void reset_lazy_tlbstate(void)
 {
-	percpu_write(cpu_tlbstate.state, 0);
-	percpu_write(cpu_tlbstate.active_mm, &init_mm);
+	this_cpu_write(cpu_tlbstate.state, 0);
+	this_cpu_write(cpu_tlbstate.active_mm, &init_mm);
 }
 #endif
 
--- head.orig/arch/x86/include/mach-xen/asm/vga.h	2011-02-01 14:39:24.000000000 +0100
+++ head/arch/x86/include/mach-xen/asm/vga.h	2012-06-14 11:23:26.000000000 +0200
@@ -17,4 +17,10 @@
 #define vga_readb(x) (*(x))
 #define vga_writeb(x, y) (*(y) = (x))
 
+#ifdef CONFIG_FB_EFI
+#define __ARCH_HAS_VGA_DEFAULT_DEVICE
+extern struct pci_dev *vga_default_device(void);
+extern void vga_set_default_device(struct pci_dev *pdev);
+#endif
+
 #endif /* _ASM_X86_VGA_H */
--- head.orig/arch/x86/include/mach-xen/asm/xor.h	2012-07-05 11:38:51.000000000 +0200
+++ head/arch/x86/include/mach-xen/asm/xor.h	2012-07-05 12:24:15.000000000 +0200
@@ -19,6 +19,7 @@ do {						\
 	xor_speed(&xor_block_32regs);		\
 	xor_speed(&xor_block_32regs_p);		\
 	xor_speed(&xor_block_sse);		\
+	AVX_XOR_SPEED;				\
 } while (0)
 
 #endif
--- head.orig/arch/x86/kernel/apic/io_apic-xen.c	2012-04-11 14:22:38.000000000 +0200
+++ head/arch/x86/kernel/apic/io_apic-xen.c	2012-06-15 13:23:22.000000000 +0200
@@ -31,6 +31,7 @@
 #include <linux/acpi.h>
 #include <linux/module.h>
 #include <linux/syscore_ops.h>
+struct msi_msg; /* #include <linux/msi.h> */
 #include <linux/freezer.h>
 #include <linux/kthread.h>
 #include <linux/jiffies.h>	/* time_after() */
@@ -51,6 +52,7 @@
 #include <asm/timer.h>
 #include <asm/i8259.h>
 #include <asm/setup.h>
+#include <asm/irq_remapping.h>
 #include <asm/hw_irq.h>
 
 #include <asm/apic.h>
@@ -76,23 +78,19 @@ unsigned long io_apic_irqs;
 #define for_each_irq_pin(entry, head) \
 	for (entry = head; entry; entry = entry->next)
 
-#ifndef CONFIG_XEN
-static void		__init __ioapic_init_mappings(void);
-
-static unsigned int	__io_apic_read  (unsigned int apic, unsigned int reg);
-static void		__io_apic_write (unsigned int apic, unsigned int reg, unsigned int val);
-static void		__io_apic_modify(unsigned int apic, unsigned int reg, unsigned int val);
-
-static struct io_apic_ops io_apic_ops = {
-	.init	= __ioapic_init_mappings,
-	.read	= __io_apic_read,
-	.write	= __io_apic_write,
-	.modify = __io_apic_modify,
-};
-
-void __init set_io_apic_ops(const struct io_apic_ops *ops)
+#ifdef CONFIG_IRQ_REMAP
+static void irq_remap_modify_chip_defaults(struct irq_chip *chip);
+static inline bool irq_remapped(struct irq_cfg *cfg)
+{
+	return cfg->irq_2_iommu.iommu != NULL;
+}
+#else
+static inline bool irq_remapped(struct irq_cfg *cfg)
+{
+	return false;
+}
+static inline void irq_remap_modify_chip_defaults(struct irq_chip *chip)
 {
-	io_apic_ops = *ops;
 }
 #endif
 
@@ -158,7 +156,7 @@ int mp_irq_entries;
 static int nr_irqs_gsi = NR_IRQS_LEGACY;
 #endif
 
-#if defined (CONFIG_MCA) || defined (CONFIG_EISA)
+#ifdef CONFIG_EISA
 int mp_bus_id_to_type[MAX_MP_BUSSES];
 #endif
 
@@ -330,21 +328,6 @@ static void free_irq_at(unsigned int at,
 	irq_free_desc(at);
 }
 
-static inline unsigned int io_apic_read(unsigned int apic, unsigned int reg)
-{
-	return io_apic_ops.read(apic, reg);
-}
-
-static inline void io_apic_write(unsigned int apic, unsigned int reg, unsigned int value)
-{
-	io_apic_ops.write(apic, reg, value);
-}
-
-static inline void io_apic_modify(unsigned int apic, unsigned int reg, unsigned int value)
-{
-	io_apic_ops.modify(apic, reg, value);
-}
-
 
 struct io_apic {
 	unsigned int index;
@@ -366,14 +349,14 @@ static inline void io_apic_eoi(unsigned 
 	writel(vector, &io_apic->eoi);
 }
 
-static unsigned int __io_apic_read(unsigned int apic, unsigned int reg)
+unsigned int native_io_apic_read(unsigned int apic, unsigned int reg)
 {
 	struct io_apic __iomem *io_apic = io_apic_base(apic);
 	writel(reg, &io_apic->index);
 	return readl(&io_apic->data);
 }
 
-static void __io_apic_write(unsigned int apic, unsigned int reg, unsigned int value)
+void native_io_apic_write(unsigned int apic, unsigned int reg, unsigned int value)
 {
 	struct io_apic __iomem *io_apic = io_apic_base(apic);
 
@@ -387,7 +370,7 @@ static void __io_apic_write(unsigned int
  *
  * Older SiS APIC requires we rewrite the index register
  */
-static void __io_apic_modify(unsigned int apic, unsigned int reg, unsigned int value)
+void native_io_apic_modify(unsigned int apic, unsigned int reg, unsigned int value)
 {
 	struct io_apic __iomem *io_apic = io_apic_base(apic);
 
@@ -395,29 +378,6 @@ static void __io_apic_modify(unsigned in
 		writel(reg, &io_apic->index);
 	writel(value, &io_apic->data);
 }
-
-static bool io_apic_level_ack_pending(struct irq_cfg *cfg)
-{
-	struct irq_pin_list *entry;
-	unsigned long flags;
-
-	raw_spin_lock_irqsave(&ioapic_lock, flags);
-	for_each_irq_pin(entry, cfg->irq_2_pin) {
-		unsigned int reg;
-		int pin;
-
-		pin = entry->pin;
-		reg = io_apic_read(entry->apic, 0x10 + pin*2);
-		/* Is the remote IRR bit set? */
-		if (reg & IO_APIC_REDIR_REMOTE_IRR) {
-			raw_spin_unlock_irqrestore(&ioapic_lock, flags);
-			return true;
-		}
-	}
-	raw_spin_unlock_irqrestore(&ioapic_lock, flags);
-
-	return false;
-}
 #else /* !CONFIG_XEN */
 static inline unsigned int io_apic_read(unsigned int apic, unsigned int reg)
 {
@@ -929,7 +889,7 @@ static int __init find_isa_irq_apic(int 
 }
 #endif
 
-#if defined(CONFIG_EISA) || defined(CONFIG_MCA)
+#ifdef CONFIG_EISA
 /*
  * EISA Edge/Level control register, ELCR
  */
@@ -966,12 +926,6 @@ static int EISA_ELCR(unsigned int irq)
 #define default_PCI_trigger(idx)	(1)
 #define default_PCI_polarity(idx)	(1)
 
-/* MCA interrupts are always polarity zero level triggered,
- * when listed as conforming in the MP table. */
-
-#define default_MCA_trigger(idx)	(1)
-#define default_MCA_polarity(idx)	default_ISA_polarity(idx)
-
 static int irq_polarity(int idx)
 {
 	int bus = mp_irqs[idx].srcbus;
@@ -1029,7 +983,7 @@ static int irq_trigger(int idx)
 				trigger = default_ISA_trigger(idx);
 			else
 				trigger = default_PCI_trigger(idx);
-#if defined(CONFIG_EISA) || defined(CONFIG_MCA)
+#ifdef CONFIG_EISA
 			switch (mp_bus_id_to_type[bus]) {
 				case MP_BUS_ISA: /* ISA pin */
 				{
@@ -1046,11 +1000,6 @@ static int irq_trigger(int idx)
 					/* set before the switch */
 					break;
 				}
-				case MP_BUS_MCA: /* MCA pin */
-				{
-					trigger = default_MCA_trigger(idx);
-					break;
-				}
 				default:
 				{
 					printk(KERN_WARNING "broken BIOS!!\n");
@@ -1305,7 +1254,7 @@ static void __clear_irq_vector(int irq, 
 	BUG_ON(!cfg->vector);
 
 	vector = cfg->vector;
-	for_each_cpu_and(cpu, cfg->domain, cpu_online_mask)
+	for_each_cpu(cpu, cfg->domain)
 		per_cpu(vector_irq, cpu)[vector] = -1;
 
 	cfg->vector = 0;
@@ -1313,7 +1262,7 @@ static void __clear_irq_vector(int irq, 
 
 	if (likely(!cfg->move_in_progress))
 		return;
-	for_each_cpu_and(cpu, cfg->old_domain, cpu_online_mask) {
+	for_each_cpu(cpu, cfg->old_domain) {
 		for (vector = FIRST_EXTERNAL_VECTOR; vector < NR_VECTORS;
 								vector++) {
 			if (per_cpu(vector_irq, cpu)[vector] != irq)
@@ -1419,69 +1368,6 @@ static void ioapic_register_intr(unsigne
 	irq_set_chip_and_handler_name(irq, chip, hdl,
 				      fasteoi ? "fasteoi" : "edge");
 }
-
-
-static int setup_ir_ioapic_entry(int irq,
-			      struct IR_IO_APIC_route_entry *entry,
-			      unsigned int destination, int vector,
-			      struct io_apic_irq_attr *attr)
-{
-	int index;
-	struct irte irte;
-	int ioapic_id = mpc_ioapic_id(attr->ioapic);
-	struct intel_iommu *iommu = map_ioapic_to_ir(ioapic_id);
-
-	if (!iommu) {
-		pr_warn("No mapping iommu for ioapic %d\n", ioapic_id);
-		return -ENODEV;
-	}
-
-	index = alloc_irte(iommu, irq, 1);
-	if (index < 0) {
-		pr_warn("Failed to allocate IRTE for ioapic %d\n", ioapic_id);
-		return -ENOMEM;
-	}
-
-	prepare_irte(&irte, vector, destination);
-
-	/* Set source-id of interrupt request */
-	set_ioapic_sid(&irte, ioapic_id);
-
-	modify_irte(irq, &irte);
-
-	apic_printk(APIC_VERBOSE, KERN_DEBUG "IOAPIC[%d]: "
-		"Set IRTE entry (P:%d FPD:%d Dst_Mode:%d "
-		"Redir_hint:%d Trig_Mode:%d Dlvry_Mode:%X "
-		"Avail:%X Vector:%02X Dest:%08X "
-		"SID:%04X SQ:%X SVT:%X)\n",
-		attr->ioapic, irte.present, irte.fpd, irte.dst_mode,
-		irte.redir_hint, irte.trigger_mode, irte.dlvry_mode,
-		irte.avail, irte.vector, irte.dest_id,
-		irte.sid, irte.sq, irte.svt);
-
-	memset(entry, 0, sizeof(*entry));
-
-	entry->index2	= (index >> 15) & 0x1;
-	entry->zero	= 0;
-	entry->format	= 1;
-	entry->index	= (index & 0x7fff);
-	/*
-	 * IO-APIC RTE will be configured with virtual vector.
-	 * irq handler will do the explicit EOI to the io-apic.
-	 */
-	entry->vector	= attr->ioapic_pin;
-	entry->mask	= 0;			/* enable IRQ */
-	entry->trigger	= attr->trigger;
-	entry->polarity	= attr->polarity;
-
-	/* Mask level triggered irqs.
-	 * Use IRQ_DELAYED_DISABLE for edge triggered irqs.
-	 */
-	if (attr->trigger)
-		entry->mask = 1;
-
-	return 0;
-}
 #else /* !CONFIG_XEN */
 #define __clear_irq_vector(irq, cfg) ((void)0)
 #define ioapic_register_intr(irq, cfg, trigger) evtchn_register_pirq(irq)
@@ -1491,12 +1377,9 @@ static int setup_ioapic_entry(int irq, s
 			       unsigned int destination, int vector,
 			       struct io_apic_irq_attr *attr)
 {
-#ifndef CONFIG_XEN
-	if (intr_remapping_enabled)
-		return setup_ir_ioapic_entry(irq,
-			 (struct IR_IO_APIC_route_entry *)entry,
-			 destination, vector, attr);
-#endif
+	if (irq_remapping_enabled)
+		return setup_ioapic_remapped_entry(irq, entry, destination,
+						   vector, attr);
 
 	memset(entry, 0, sizeof(*entry));
 
@@ -1671,7 +1554,7 @@ static void __init setup_timer_IRQ0_pin(
 {
 	struct IO_APIC_route_entry entry;
 
-	if (intr_remapping_enabled)
+	if (irq_remapping_enabled)
 		return;
 
 	memset(&entry, 0, sizeof(entry));
@@ -1757,7 +1640,7 @@ __apicdebuginit(void) print_IO_APIC(int 
 
 	printk(KERN_DEBUG ".... IRQ redirection table:\n");
 
-	if (intr_remapping_enabled) {
+	if (irq_remapping_enabled) {
 		printk(KERN_DEBUG " NR Indx Fmt Mask Trig IRR"
 			" Pol Stat Indx2 Zero Vect:\n");
 	} else {
@@ -1766,7 +1649,7 @@ __apicdebuginit(void) print_IO_APIC(int 
 	}
 
 	for (i = 0; i <= reg_01.bits.entries; i++) {
-		if (intr_remapping_enabled) {
+		if (irq_remapping_enabled) {
 			struct IO_APIC_route_entry entry;
 			struct IR_IO_APIC_route_entry *ir_entry;
 
@@ -2133,7 +2016,7 @@ void disable_IO_APIC(void)
 	 * IOAPIC RTE as well as interrupt-remapping table entry).
 	 * As this gets called during crash dump, keep this simple for now.
 	 */
-	if (ioapic_i8259.pin != -1 && !intr_remapping_enabled) {
+	if (ioapic_i8259.pin != -1 && !irq_remapping_enabled) {
 		struct IO_APIC_route_entry entry;
 
 		memset(&entry, 0, sizeof(entry));
@@ -2157,7 +2040,7 @@ void disable_IO_APIC(void)
 	 * Use virtual wire A mode when interrupt remapping is enabled.
 	 */
 	if (cpu_has_apic || apic_from_smp_config())
-		disconnect_bsp_APIC(!intr_remapping_enabled &&
+		disconnect_bsp_APIC(!irq_remapping_enabled &&
 				ioapic_i8259.pin != -1);
 }
 
@@ -2473,71 +2356,6 @@ ioapic_set_affinity(struct irq_data *dat
 	return ret;
 }
 
-#ifdef CONFIG_IRQ_REMAP
-
-/*
- * Migrate the IO-APIC irq in the presence of intr-remapping.
- *
- * For both level and edge triggered, irq migration is a simple atomic
- * update(of vector and cpu destination) of IRTE and flush the hardware cache.
- *
- * For level triggered, we eliminate the io-apic RTE modification (with the
- * updated vector information), by using a virtual vector (io-apic pin number).
- * Real vector that is used for interrupting cpu will be coming from
- * the interrupt-remapping table entry.
- *
- * As the migration is a simple atomic update of IRTE, the same mechanism
- * is used to migrate MSI irq's in the presence of interrupt-remapping.
- */
-static int
-ir_ioapic_set_affinity(struct irq_data *data, const struct cpumask *mask,
-		       bool force)
-{
-	struct irq_cfg *cfg = data->chip_data;
-	unsigned int dest, irq = data->irq;
-	struct irte irte;
-
-	if (!cpumask_intersects(mask, cpu_online_mask))
-		return -EINVAL;
-
-	if (get_irte(irq, &irte))
-		return -EBUSY;
-
-	if (assign_irq_vector(irq, cfg, mask))
-		return -EBUSY;
-
-	dest = apic->cpu_mask_to_apicid_and(cfg->domain, mask);
-
-	irte.vector = cfg->vector;
-	irte.dest_id = IRTE_DEST(dest);
-
-	/*
-	 * Atomically updates the IRTE with the new destination, vector
-	 * and flushes the interrupt entry cache.
-	 */
-	modify_irte(irq, &irte);
-
-	/*
-	 * After this point, all the interrupts will start arriving
-	 * at the new destination. So, time to cleanup the previous
-	 * vector allocation.
-	 */
-	if (cfg->move_in_progress)
-		send_cleanup_vector(cfg);
-
-	cpumask_copy(data->affinity, mask);
-	return 0;
-}
-
-#else
-static inline int
-ir_ioapic_set_affinity(struct irq_data *data, const struct cpumask *mask,
-		       bool force)
-{
-	return 0;
-}
-#endif
-
 asmlinkage void smp_irq_move_cleanup_interrupt(void)
 {
 	unsigned vector, me;
@@ -2635,6 +2453,31 @@ static void ack_apic_edge(struct irq_dat
 atomic_t irq_mis_count;
 
 #ifdef CONFIG_GENERIC_PENDING_IRQ
+#ifndef CONFIG_XEN
+static bool io_apic_level_ack_pending(struct irq_cfg *cfg)
+{
+	struct irq_pin_list *entry;
+	unsigned long flags;
+
+	raw_spin_lock_irqsave(&ioapic_lock, flags);
+	for_each_irq_pin(entry, cfg->irq_2_pin) {
+		unsigned int reg;
+		int pin;
+
+		pin = entry->pin;
+		reg = io_apic_read(entry->apic, 0x10 + pin*2);
+		/* Is the remote IRR bit set? */
+		if (reg & IO_APIC_REDIR_REMOTE_IRR) {
+			raw_spin_unlock_irqrestore(&ioapic_lock, flags);
+			return true;
+		}
+	}
+	raw_spin_unlock_irqrestore(&ioapic_lock, flags);
+
+	return false;
+}
+#endif
+
 static inline bool ioapic_irqd_mask(struct irq_data *data, struct irq_cfg *cfg)
 {
 	/* If we are moving the irq we need to mask it */
@@ -2782,7 +2625,7 @@ static void irq_remap_modify_chip_defaul
 	chip->irq_eoi = ir_ack_apic_level;
 
 #ifdef CONFIG_SMP
-	chip->irq_set_affinity = ir_ioapic_set_affinity;
+	chip->irq_set_affinity = set_remapped_irq_affinity;
 #endif
 }
 #endif /* CONFIG_IRQ_REMAP */
@@ -3001,7 +2844,7 @@ static inline void __init check_timer(vo
 	 * 8259A.
 	 */
 	if (pin1 == -1) {
-		if (intr_remapping_enabled)
+		if (irq_remapping_enabled)
 			panic("BIOS bug: timer not connected to IO-APIC");
 		pin1 = pin2;
 		apic1 = apic2;
@@ -3034,7 +2877,7 @@ static inline void __init check_timer(vo
 				clear_IO_APIC_pin(0, pin1);
 			goto out;
 		}
-		if (intr_remapping_enabled)
+		if (irq_remapping_enabled)
 			panic("timer doesn't work through Interrupt-remapped IO-APIC");
 		local_irq_disable();
 		clear_IO_APIC_pin(apic1, pin1);
@@ -3272,7 +3115,7 @@ void destroy_irq(unsigned int irq)
 	irq_set_status_flags(irq, IRQ_NOREQUEST|IRQ_NOPROBE);
 
 	if (irq_remapped(cfg))
-		free_irte(irq);
+		free_remapped_irq(irq);
 	raw_spin_lock_irqsave(&vector_lock, flags);
 	__clear_irq_vector(irq, cfg);
 	raw_spin_unlock_irqrestore(&vector_lock, flags);
@@ -3302,54 +3145,34 @@ static int msi_compose_msg(struct pci_de
 	dest = apic->cpu_mask_to_apicid_and(cfg->domain, apic->target_cpus());
 
 	if (irq_remapped(cfg)) {
-		struct irte irte;
-		int ir_index;
-		u16 sub_handle;
-
-		ir_index = map_irq_to_irte_handle(irq, &sub_handle);
-		BUG_ON(ir_index == -1);
-
-		prepare_irte(&irte, cfg->vector, dest);
-
-		/* Set source-id of interrupt request */
-		if (pdev)
-			set_msi_sid(&irte, pdev);
-		else
-			set_hpet_sid(&irte, hpet_id);
-
-		modify_irte(irq, &irte);
+		compose_remapped_msi_msg(pdev, irq, dest, msg, hpet_id);
+		return err;
+	}
 
+	if (x2apic_enabled())
+		msg->address_hi = MSI_ADDR_BASE_HI |
+				  MSI_ADDR_EXT_DEST_ID(dest);
+	else
 		msg->address_hi = MSI_ADDR_BASE_HI;
-		msg->data = sub_handle;
-		msg->address_lo = MSI_ADDR_BASE_LO | MSI_ADDR_IR_EXT_INT |
-				  MSI_ADDR_IR_SHV |
-				  MSI_ADDR_IR_INDEX1(ir_index) |
-				  MSI_ADDR_IR_INDEX2(ir_index);
-	} else {
-		if (x2apic_enabled())
-			msg->address_hi = MSI_ADDR_BASE_HI |
-					  MSI_ADDR_EXT_DEST_ID(dest);
-		else
-			msg->address_hi = MSI_ADDR_BASE_HI;
 
-		msg->address_lo =
-			MSI_ADDR_BASE_LO |
-			((apic->irq_dest_mode == 0) ?
-				MSI_ADDR_DEST_MODE_PHYSICAL:
-				MSI_ADDR_DEST_MODE_LOGICAL) |
-			((apic->irq_delivery_mode != dest_LowestPrio) ?
-				MSI_ADDR_REDIRECTION_CPU:
-				MSI_ADDR_REDIRECTION_LOWPRI) |
-			MSI_ADDR_DEST_ID(dest);
-
-		msg->data =
-			MSI_DATA_TRIGGER_EDGE |
-			MSI_DATA_LEVEL_ASSERT |
-			((apic->irq_delivery_mode != dest_LowestPrio) ?
-				MSI_DATA_DELIVERY_FIXED:
-				MSI_DATA_DELIVERY_LOWPRI) |
-			MSI_DATA_VECTOR(cfg->vector);
-	}
+	msg->address_lo =
+		MSI_ADDR_BASE_LO |
+		((apic->irq_dest_mode == 0) ?
+			MSI_ADDR_DEST_MODE_PHYSICAL:
+			MSI_ADDR_DEST_MODE_LOGICAL) |
+		((apic->irq_delivery_mode != dest_LowestPrio) ?
+			MSI_ADDR_REDIRECTION_CPU:
+			MSI_ADDR_REDIRECTION_LOWPRI) |
+		MSI_ADDR_DEST_ID(dest);
+
+	msg->data =
+		MSI_DATA_TRIGGER_EDGE |
+		MSI_DATA_LEVEL_ASSERT |
+		((apic->irq_delivery_mode != dest_LowestPrio) ?
+			MSI_DATA_DELIVERY_FIXED:
+			MSI_DATA_DELIVERY_LOWPRI) |
+		MSI_DATA_VECTOR(cfg->vector);
+
 	return err;
 }
 
@@ -3392,33 +3215,6 @@ static struct irq_chip msi_chip = {
 	.irq_retrigger		= ioapic_retrigger_irq,
 };
 
-/*
- * Map the PCI dev to the corresponding remapping hardware unit
- * and allocate 'nvec' consecutive interrupt-remapping table entries
- * in it.
- */
-static int msi_alloc_irte(struct pci_dev *dev, int irq, int nvec)
-{
-	struct intel_iommu *iommu;
-	int index;
-
-	iommu = map_dev_to_ir(dev);
-	if (!iommu) {
-		printk(KERN_ERR
-		       "Unable to map PCI %s to iommu\n", pci_name(dev));
-		return -ENOENT;
-	}
-
-	index = alloc_irte(iommu, irq, nvec);
-	if (index < 0) {
-		printk(KERN_ERR
-		       "Unable to allocate %d IRTE for PCI %s\n", nvec,
-		       pci_name(dev));
-		return -ENOSPC;
-	}
-	return index;
-}
-
 static int setup_msi_irq(struct pci_dev *dev, struct msi_desc *msidesc, int irq)
 {
 	struct irq_chip *chip = &msi_chip;
@@ -3449,7 +3245,6 @@ int native_setup_msi_irqs(struct pci_dev
 	int node, ret, sub_handle, index = 0;
 	unsigned int irq, irq_want;
 	struct msi_desc *msidesc;
-	struct intel_iommu *iommu = NULL;
 
 	/* x86 doesn't support multiple MSI yet */
 	if (type == PCI_CAP_ID_MSI && nvec > 1)
@@ -3463,7 +3258,7 @@ int native_setup_msi_irqs(struct pci_dev
 		if (irq == 0)
 			return -1;
 		irq_want = irq + 1;
-		if (!intr_remapping_enabled)
+		if (!irq_remapping_enabled)
 			goto no_ir;
 
 		if (!sub_handle) {
@@ -3471,23 +3266,16 @@ int native_setup_msi_irqs(struct pci_dev
 			 * allocate the consecutive block of IRTE's
 			 * for 'nvec'
 			 */
-			index = msi_alloc_irte(dev, irq, nvec);
+			index = msi_alloc_remapped_irq(dev, irq, nvec);
 			if (index < 0) {
 				ret = index;
 				goto error;
 			}
 		} else {
-			iommu = map_dev_to_ir(dev);
-			if (!iommu) {
-				ret = -ENOENT;
+			ret = msi_setup_remapped_irq(dev, irq, index,
+						     sub_handle);
+			if (ret < 0)
 				goto error;
-			}
-			/*
-			 * setup the mapping between the irq and the IRTE
-			 * base index, the sub_handle pointing to the
-			 * appropriate interrupt remap table entry.
-			 */
-			set_irte_irq(irq, iommu, index, sub_handle);
 		}
 no_ir:
 		ret = setup_msi_irq(dev, msidesc, irq);
@@ -3605,15 +3393,8 @@ int arch_setup_hpet_msi(unsigned int irq
 	struct msi_msg msg;
 	int ret;
 
-	if (intr_remapping_enabled) {
-		struct intel_iommu *iommu = map_hpet_to_ir(id);
-		int index;
-
-		if (!iommu)
-			return -1;
-
-		index = alloc_irte(iommu, irq, 1);
-		if (index < 0)
+	if (irq_remapping_enabled) {
+		if (!setup_hpet_msi_remapped(irq, id))
 			return -1;
 	}
 
@@ -4006,8 +3787,8 @@ void __init setup_ioapic_dest(void)
 		else
 			mask = apic->target_cpus();
 
-		if (intr_remapping_enabled)
-			ir_ioapic_set_affinity(idata, mask, false);
+		if (irq_remapping_enabled)
+			set_remapped_irq_affinity(idata, mask, false);
 		else
 			ioapic_set_affinity(idata, mask, false);
 	}
@@ -4049,12 +3830,7 @@ static struct resource * __init ioapic_s
 	return res;
 }
 
-void __init ioapic_and_gsi_init(void)
-{
-	io_apic_ops.init();
-}
-
-static void __init __ioapic_init_mappings(void)
+void __init native_io_apic_init_mappings(void)
 {
 	unsigned long ioapic_phys, idx = FIX_IO_APIC_BASE_0;
 	struct resource *ioapic_res;
--- head.orig/arch/x86/kernel/cpu/common-xen.c	2012-08-01 12:13:15.000000000 +0200
+++ head/arch/x86/kernel/cpu/common-xen.c	2012-08-01 12:17:49.000000000 +0200
@@ -390,7 +390,7 @@ void load_percpu_segment(int cpu)
 #endif
 #endif
 #ifdef CONFIG_XEN
-	percpu_write(xen_x86_cr0, native_read_cr0());
+	__this_cpu_write(xen_x86_cr0, native_read_cr0());
 	xen_clear_cr0_upd();
 #endif
 	load_stack_canary_segment();
@@ -1189,14 +1189,20 @@ int is_debug_stack(unsigned long addr)
 		 addr > (__get_cpu_var(debug_stack_addr) - DEBUG_STKSZ));
 }
 
+static DEFINE_PER_CPU(u32, debug_stack_use_ctr);
+
 void debug_stack_set_zero(void)
 {
+	this_cpu_inc(debug_stack_use_ctr);
 	load_idt((const struct desc_ptr *)&nmi_idt_descr);
 }
 
 void debug_stack_reset(void)
 {
-	load_idt((const struct desc_ptr *)&idt_descr);
+	if (WARN_ON(!this_cpu_read(debug_stack_use_ctr)))
+		return;
+	if (this_cpu_dec_return(debug_stack_use_ctr) == 0)
+		load_idt((const struct desc_ptr *)&idt_descr);
 }
 #endif
 
@@ -1281,7 +1287,7 @@ void __cpuinit cpu_init(void)
 #endif
 
 #ifdef CONFIG_NUMA
-	if (cpu != 0 && percpu_read(numa_node) == 0 &&
+	if (cpu != 0 && this_cpu_read(numa_node) == 0 &&
 	    early_cpu_to_node(cpu) != NUMA_NO_NODE)
 		set_numa_node(early_cpu_to_node(cpu));
 #endif
--- head.orig/arch/x86/kernel/cpu/mcheck/mce.c	2012-08-20 13:22:05.000000000 +0200
+++ head/arch/x86/kernel/cpu/mcheck/mce.c	2012-08-20 13:22:57.000000000 +0200
@@ -1272,7 +1272,7 @@ void mce_log_therm_throt_event(__u64 sta
  * mechanism will be done in XEN for Intel CPUs
  */
 #if defined (CONFIG_X86_XEN_MCE)
-static int check_interval = 0; /* disable polling */
+static unsigned long check_interval = 0; /* disable polling */
 #else
 static unsigned long check_interval = 5 * 60; /* 5 minutes */
 #endif
--- head.orig/arch/x86/kernel/e820-xen.c	2012-02-16 17:12:00.000000000 +0100
+++ head/arch/x86/kernel/e820-xen.c	2012-06-14 11:23:26.000000000 +0200
@@ -136,7 +136,9 @@ static void __init __e820_add_region(str
 	int x = e820x->nr_map;
 
 	if (x >= ARRAY_SIZE(e820x->map)) {
-		printk(KERN_ERR "Ooops! Too many entries in the memory map!\n");
+		printk(KERN_ERR "e820: too many entries; ignoring [mem %#010llx-%#010llx]\n",
+		       (unsigned long long) start,
+		       (unsigned long long) (start + size - 1));
 		return;
 	}
 
@@ -156,19 +158,19 @@ static void __init e820_print_type(u32 t
 	switch (type) {
 	case E820_RAM:
 	case E820_RESERVED_KERN:
-		printk(KERN_CONT "(usable)");
+		printk(KERN_CONT "usable");
 		break;
 	case E820_RESERVED:
-		printk(KERN_CONT "(reserved)");
+		printk(KERN_CONT "reserved");
 		break;
 	case E820_ACPI:
-		printk(KERN_CONT "(ACPI data)");
+		printk(KERN_CONT "ACPI data");
 		break;
 	case E820_NVS:
-		printk(KERN_CONT "(ACPI NVS)");
+		printk(KERN_CONT "ACPI NVS");
 		break;
 	case E820_UNUSABLE:
-		printk(KERN_CONT "(unusable)");
+		printk(KERN_CONT "unusable");
 		break;
 	default:
 		printk(KERN_CONT "type %u", type);
@@ -181,10 +183,10 @@ static void __init _e820_print_map(const
 	int i;
 
 	for (i = 0; i < e820->nr_map; i++) {
-		printk(KERN_INFO " %s: %016Lx - %016Lx ", who,
+		printk(KERN_INFO "%s: [mem %#018Lx-%#018Lx] ", who,
 		       (unsigned long long) e820->map[i].addr,
 		       (unsigned long long)
-		       (e820->map[i].addr + e820->map[i].size));
+		       (e820->map[i].addr + e820->map[i].size - 1));
 		e820_print_type(e820->map[i].type);
 		printk(KERN_CONT "\n");
 	}
@@ -459,9 +461,8 @@ static u64 __init __e820_update_range(st
 		size = ULLONG_MAX - start;
 
 	end = start + size;
-	printk(KERN_DEBUG "e820 update range: %016Lx - %016Lx ",
-		       (unsigned long long) start,
-		       (unsigned long long) end);
+	printk(KERN_DEBUG "e820: update [mem %#010Lx-%#010Lx] ",
+	       (unsigned long long) start, (unsigned long long) (end - 1));
 	e820_print_type(old_type);
 	printk(KERN_CONT " ==> ");
 	e820_print_type(new_type);
@@ -549,9 +550,8 @@ u64 __init e820_remove_range(u64 start, 
 		size = ULLONG_MAX - start;
 
 	end = start + size;
-	printk(KERN_DEBUG "e820 remove range: %016Lx - %016Lx ",
-		       (unsigned long long) start,
-		       (unsigned long long) end);
+	printk(KERN_DEBUG "e820: remove [mem %#010Lx-%#010Lx] ",
+	       (unsigned long long) start, (unsigned long long) (end - 1));
 	if (checktype)
 		e820_print_type(old_type);
 	printk(KERN_CONT "\n");
@@ -607,7 +607,7 @@ void __init update_e820(void)
 	if (sanitize_e820_map(e820.map, ARRAY_SIZE(e820.map), &nr_map))
 		return;
 	e820.nr_map = nr_map;
-	printk(KERN_INFO "modified physical RAM map:\n");
+	printk(KERN_INFO "e820: modified physical RAM map:\n");
 	_e820_print_map(&e820, "modified");
 }
 #ifndef CONFIG_XEN_UNPRIVILEGED_GUEST
@@ -687,8 +687,8 @@ __init void e820_setup_gap(void)
 #ifdef CONFIG_X86_64
 	if (!found) {
 		printk(KERN_ERR
-	"PCI: Warning: Cannot find a gap in the 32bit address range\n"
-	"PCI: Unassigned devices with 32bit resource registers may break!\n");
+	"e820: cannot find a gap in the 32bit address range\n"
+	"e820: PCI devices with unassigned 32bit BARs may break!\n");
 		found = e820_search_gap(&gapstart, &gapsize, MAX_GAP_END, 0);
 		WARN_ON(!found);
 	}
@@ -700,8 +700,8 @@ __init void e820_setup_gap(void)
 	pci_mem_start = gapstart;
 
 	printk(KERN_INFO
-	       "Allocating PCI resources starting at %lx (gap: %lx:%lx)\n",
-	       pci_mem_start, gapstart, gapsize);
+	       "e820: [mem %#010lx-%#010lx] available for PCI devices\n",
+	       gapstart, gapstart + gapsize - 1);
 }
 
 #undef e820
@@ -722,7 +722,7 @@ void __init parse_e820_ext(struct setup_
 	extmap = (struct e820entry *)(sdata->data);
 	__append_e820_map(extmap, entries);
 	sanitize_e820_map(e820.map, ARRAY_SIZE(e820.map), &e820.nr_map);
-	printk(KERN_INFO "extended physical RAM map:\n");
+	printk(KERN_INFO "e820: extended physical RAM map:\n");
 	_e820_print_map(&e820, "extended");
 }
 
@@ -801,7 +801,7 @@ u64 __init early_reserve_e820(u64 size, 
 	addr = __memblock_alloc_base(size, align, MEMBLOCK_ALLOC_ACCESSIBLE);
 	if (addr) {
 		e820_update_range_saved(addr, size, E820_RAM, E820_RESERVED);
-		printk(KERN_INFO "update e820_saved for early_reserve_e820\n");
+		printk(KERN_INFO "e820: update e820_saved for early_reserve_e820\n");
 		update_e820_saved();
 	}
 #ifdef CONFIG_XEN
@@ -878,7 +878,7 @@ static unsigned long __init e820_end_pfn
 	if (last_pfn > max_arch_pfn)
 		last_pfn = max_arch_pfn;
 
-	printk(KERN_INFO "last_pfn = %#lx max_arch_pfn = %#lx\n",
+	printk(KERN_INFO "e820: last_pfn = %#lx max_arch_pfn = %#lx\n",
 			 last_pfn, max_arch_pfn);
 	return last_pfn;
 }
@@ -1000,7 +1000,7 @@ void __init finish_e820_parsing(void)
 			early_panic("Invalid user supplied memory map");
 		e820.nr_map = nr;
 
-		printk(KERN_INFO "user-defined physical RAM map:\n");
+		printk(KERN_INFO "e820: user-defined physical RAM map:\n");
 		_e820_print_map(&e820, "user");
 	}
 }
@@ -1112,8 +1112,9 @@ void __init e820_reserve_resources_late(
 			end = MAX_RESOURCE_SIZE;
 		if (start >= end)
 			continue;
-		printk(KERN_DEBUG "reserve RAM buffer: %016llx - %016llx ",
-			       start, end);
+		printk(KERN_DEBUG
+		       "e820: reserve RAM buffer [mem %#010llx-%#010llx]\n",
+		       start, end);
 		reserve_region_with_split(&iomem_resource, start, end,
 					  "RAM buffer");
 	}
@@ -1176,7 +1177,7 @@ void __init setup_memory_map(void)
 #endif
 		memcpy(&e820_saved, &e820, sizeof(struct e820map));
 #endif
-	printk(KERN_INFO "Xen-provided physical RAM map:\n");
+	printk(KERN_INFO "e820: Xen-provided physical RAM map:\n");
 	_e820_print_map(&e820, who);
 }
 
--- head.orig/arch/x86/kernel/entry_32-xen.S	2012-04-11 13:26:23.000000000 +0200
+++ head/arch/x86/kernel/entry_32-xen.S	2012-06-14 12:11:29.000000000 +0200
@@ -56,6 +56,7 @@
 #include <asm/irq_vectors.h>
 #include <asm/cpufeature.h>
 #include <asm/alternative-asm.h>
+#include <asm/asm.h>
 #include <xen/interface/xen.h>
 
 /* Avoid __ASSEMBLER__'ifying <linux/audit.h> just for this.  */
@@ -155,10 +156,8 @@ NMI_MASK	= 0x80000000
 .pushsection .fixup, "ax"
 99:	movl $0, (%esp)
 	jmp 98b
-.section __ex_table, "a"
-	.align 4
-	.long 98b, 99b
 .popsection
+	_ASM_EXTABLE(98b,99b)
 .endm
 
 .macro PTGS_TO_GS
@@ -168,10 +167,8 @@ NMI_MASK	= 0x80000000
 .pushsection .fixup, "ax"
 99:	movl $0, PT_GS(%esp)
 	jmp 98b
-.section __ex_table, "a"
-	.align 4
-	.long 98b, 99b
 .popsection
+	_ASM_EXTABLE(98b,99b)
 .endm
 
 .macro GS_TO_REG reg
@@ -253,12 +250,10 @@ NMI_MASK	= 0x80000000
 	jmp 2b
 6:	movl $0, (%esp)
 	jmp 3b
-.section __ex_table, "a"
-	.align 4
-	.long 1b, 4b
-	.long 2b, 5b
-	.long 3b, 6b
 .popsection
+	_ASM_EXTABLE(1b,4b)
+	_ASM_EXTABLE(2b,5b)
+	_ASM_EXTABLE(3b,6b)
 	POP_GS_EX
 .endm
 
@@ -325,7 +320,6 @@ ret_from_exception:
 	preempt_stop(CLBR_ANY)
 ret_from_intr:
 	GET_THREAD_INFO(%ebp)
-resume_userspace_sig:
 #ifdef CONFIG_VM86
 	movl PT_EFLAGS(%esp), %eax	# mix EFLAGS and CS
 	movb PT_CS(%esp), %al
@@ -419,10 +413,7 @@ sysenter_past_esp:
 	jae syscall_fault
 1:	movl (%ebp),%ebp
 	movl %ebp,PT_EBP(%esp)
-.section __ex_table,"a"
-	.align 4
-	.long 1b,syscall_fault
-.previous
+	_ASM_EXTABLE(1b,syscall_fault)
 
 	GET_THREAD_INFO(%ebp)
 
@@ -489,10 +480,8 @@ sysexit_audit:
 .pushsection .fixup,"ax"
 2:	movl $0,PT_FS(%esp)
 	jmp 1b
-.section __ex_table,"a"
-	.align 4
-	.long 1b,2b
 .popsection
+	_ASM_EXTABLE(1b,2b)
 	PTGS_TO_GS_EX
 ENDPROC(ia32_sysenter_target)
 
@@ -513,10 +502,7 @@ ENTRY(ia32pv_sysenter_target)
 	cmpl $__PAGE_OFFSET-3,%ebp
 	jae syscall_fault
 1:	movl (%ebp),%ebp
-.section __ex_table,"a"
-	.align 4
-	.long 1b,syscall_fault
-.previous
+	_ASM_EXTABLE(1b,syscall_fault)
 	jmp system_call
 	CFI_ENDPROC
 ENDPROC(ia32pv_sysenter_target)
@@ -586,10 +572,7 @@ ENTRY(iret_exc)
 	pushl $do_iret_error
 	jmp error_code
 .previous
-.section __ex_table,"a"
-	.align 4
-	.long irq_return,iret_exc
-.previous
+	_ASM_EXTABLE(irq_return,iret_exc)
 
 	CFI_RESTORE_STATE
 #ifndef CONFIG_XEN
@@ -650,10 +633,7 @@ scrit:	/**** START OF CRITICAL REGION **
 	jnz  14f			# process more events if necessary...
 	RESTORE_REGS 4
 1:	INTERRUPT_RETURN
-.section __ex_table,"a"
-	.align 4
-	.long 1b,iret_exc
-.previous
+	_ASM_EXTABLE(1b,iret_exc)
 14:	__DISABLE_INTERRUPTS
 	TRACE_IRQS_OFF
 ecrit:  /**** END OF CRITICAL REGION ****/
@@ -697,9 +677,13 @@ work_notifysig:				# deal with pending s
 					# vm86-space
 	TRACE_IRQS_ON
 	ENABLE_INTERRUPTS(CLBR_NONE)
+	movb PT_CS(%esp), %bl
+	andb $SEGMENT_RPL_MASK, %bl
+	cmpb $USER_RPL, %bl
+	jb resume_kernel
 	xorl %edx, %edx
 	call do_notify_resume
-	jmp resume_userspace_sig
+	jmp resume_userspace
 
 	ALIGN
 work_notifysig_v86:
@@ -712,9 +696,13 @@ work_notifysig_v86:
 #endif
 	TRACE_IRQS_ON
 	ENABLE_INTERRUPTS(CLBR_NONE)
+	movb PT_CS(%esp), %bl
+	andb $SEGMENT_RPL_MASK, %bl
+	cmpb $USER_RPL, %bl
+	jb resume_kernel
 	xorl %edx, %edx
 	call do_notify_resume
-	jmp resume_userspace_sig
+	jmp resume_userspace
 END(work_pending)
 
 	# perform syscall exit tracing
@@ -1066,14 +1054,11 @@ ENTRY(failsafe_callback)
 9:	xorl %eax,%eax;		\
 	movl %eax,16(%esp);	\
 	jmp 4b;			\
-.previous;			\
-.section __ex_table,"a";	\
-	.align 4;		\
-	.long 1b,6b;		\
-	.long 2b,7b;		\
-	.long 3b,8b;		\
-	.long 4b,9b;		\
 .previous
+	_ASM_EXTABLE(1b,6b)
+	_ASM_EXTABLE(2b,7b)
+	_ASM_EXTABLE(3b,8b)
+	_ASM_EXTABLE(4b,9b)
 #endif
 	CFI_ENDPROC
 
@@ -1117,10 +1102,7 @@ END(device_not_available)
 #ifdef CONFIG_PARAVIRT
 ENTRY(native_iret)
 	iret
-.section __ex_table,"a"
-	.align 4
-	.long native_iret, iret_exc
-.previous
+	_ASM_EXTABLE(native_iret, iret_exc)
 END(native_iret)
 
 ENTRY(native_irq_enable_sysexit)
@@ -1360,10 +1342,7 @@ ENTRY(ia32pv_cstar_target)
 	CFI_REMEMBER_STATE
 	ja cstar_fault
 1:	movl (%ebp),%ebp
-.section __ex_table,"a"
-	.align 4
-	.long 1b,cstar_fault
-.previous
+	_ASM_EXTABLE(1b,cstar_fault)
 	SAVE_ALL
 	GET_THREAD_INFO(%ebp)
 	testl $_TIF_WORK_SYSCALL_ENTRY,TI_flags(%ebp)
--- head.orig/arch/x86/kernel/entry_64-xen.S	2012-04-11 13:26:23.000000000 +0200
+++ head/arch/x86/kernel/entry_64-xen.S	2012-06-14 11:23:26.000000000 +0200
@@ -58,6 +58,7 @@
 #include <asm/processor-flags.h>
 #include <asm/ftrace.h>
 #include <asm/percpu.h>
+#include <asm/asm.h>
 #include <linux/err.h>
 #include <xen/interface/xen.h>
 #include <xen/interface/features.h>
@@ -187,6 +188,44 @@ GLOBAL(return_to_handler)
 #endif
 .endm
 
+/*
+ * When dynamic function tracer is enabled it will add a breakpoint
+ * to all locations that it is about to modify, sync CPUs, update
+ * all the code, sync CPUs, then remove the breakpoints. In this time
+ * if lockdep is enabled, it might jump back into the debug handler
+ * outside the updating of the IST protection. (TRACE_IRQS_ON/OFF).
+ *
+ * We need to change the IDT table before calling TRACE_IRQS_ON/OFF to
+ * make sure the stack pointer does not get reset back to the top
+ * of the debug stack, and instead just reuses the current stack.
+ */
+#if defined(CONFIG_DYNAMIC_FTRACE) && defined(CONFIG_TRACE_IRQFLAGS)
+
+.macro TRACE_IRQS_OFF_DEBUG
+	call debug_stack_set_zero
+	TRACE_IRQS_OFF
+	call debug_stack_reset
+.endm
+
+.macro TRACE_IRQS_ON_DEBUG
+	call debug_stack_set_zero
+	TRACE_IRQS_ON
+	call debug_stack_reset
+.endm
+
+.macro TRACE_IRQS_IRETQ_DEBUG offset=ARGOFFSET
+	bt   $9,EFLAGS-\offset(%rsp)	/* interrupts off? */
+	jnc  1f
+	TRACE_IRQS_ON_DEBUG
+1:
+.endm
+
+#else
+# define TRACE_IRQS_OFF_DEBUG		TRACE_IRQS_OFF
+# define TRACE_IRQS_ON_DEBUG		TRACE_IRQS_ON
+# define TRACE_IRQS_IRETQ_DEBUG		TRACE_IRQS_IRETQ
+#endif
+
 NMI_MASK = 0x80000000
 	
 /*
@@ -1221,7 +1260,7 @@ paranoidzeroentry machine_check *machine
 ENTRY(paranoid_exit)
 	DEFAULT_FRAME
 	DISABLE_INTERRUPTS(CLBR_NONE)
-	TRACE_IRQS_OFF
+	TRACE_IRQS_OFF_DEBUG
 	testl %ebx,%ebx				/* swapgs needed? */
 	jnz paranoid_restore
 	testl $3,CS(%rsp)
@@ -1232,7 +1271,7 @@ paranoid_swapgs:
 	RESTORE_ALL 8
 	jmp irq_return
 paranoid_restore:
-	TRACE_IRQS_IRETQ 0
+	TRACE_IRQS_IRETQ_DEBUG 0
 	RESTORE_ALL 8
 	jmp irq_return
 paranoid_userspace:
--- head.orig/arch/x86/kernel/head32-xen.c	2012-02-09 12:32:50.000000000 +0100
+++ head/arch/x86/kernel/head32-xen.c	2012-06-14 11:23:26.000000000 +0200
@@ -13,7 +13,6 @@
 #include <asm/setup.h>
 #include <asm/sections.h>
 #include <asm/e820.h>
-#include <asm/trampoline.h>
 #include <asm/apic.h>
 #include <asm/io_apic.h>
 #include <asm/tlbflush.h>
--- head.orig/arch/x86/kernel/head64-xen.c	2012-02-09 12:32:50.000000000 +0100
+++ head/arch/x86/kernel/head64-xen.c	2012-06-14 11:23:26.000000000 +0200
@@ -27,7 +27,6 @@
 #include <asm/sections.h>
 #include <asm/kdebug.h>
 #include <asm/e820.h>
-#include <asm/trampoline.h>
 #include <asm/bios_ebda.h>
 
 #ifndef CONFIG_XEN
--- head.orig/arch/x86/kernel/mpparse-xen.c	2012-02-09 12:32:50.000000000 +0100
+++ head/arch/x86/kernel/mpparse-xen.c	2012-06-14 11:23:26.000000000 +0200
@@ -27,7 +27,6 @@
 #include <asm/proto.h>
 #include <asm/bios_ebda.h>
 #include <asm/e820.h>
-#include <asm/trampoline.h>
 #include <asm/setup.h>
 #include <asm/smp.h>
 
@@ -109,7 +108,7 @@ static void __init MP_bus_info(struct mp
 
 	set_bit(m->busid, mp_bus_not_pci);
 	if (strncmp(str, BUSTYPE_ISA, sizeof(BUSTYPE_ISA) - 1) == 0) {
-#if defined(CONFIG_EISA) || defined(CONFIG_MCA)
+#ifdef CONFIG_EISA
 		mp_bus_id_to_type[m->busid] = MP_BUS_ISA;
 #endif
 	} else if (strncmp(str, BUSTYPE_PCI, sizeof(BUSTYPE_PCI) - 1) == 0) {
@@ -117,12 +116,10 @@ static void __init MP_bus_info(struct mp
 			x86_init.mpparse.mpc_oem_pci_bus(m);
 
 		clear_bit(m->busid, mp_bus_not_pci);
-#if defined(CONFIG_EISA) || defined(CONFIG_MCA)
+#ifdef CONFIG_EISA
 		mp_bus_id_to_type[m->busid] = MP_BUS_PCI;
 	} else if (strncmp(str, BUSTYPE_EISA, sizeof(BUSTYPE_EISA) - 1) == 0) {
 		mp_bus_id_to_type[m->busid] = MP_BUS_EISA;
-	} else if (strncmp(str, BUSTYPE_MCA, sizeof(BUSTYPE_MCA) - 1) == 0) {
-		mp_bus_id_to_type[m->busid] = MP_BUS_MCA;
 #endif
 	} else
 		printk(KERN_WARNING "Unknown bustype %s - ignoring\n", str);
@@ -384,9 +381,6 @@ static void __init construct_ioapic_tabl
 	case 3:
 		memcpy(bus.bustype, "EISA  ", 6);
 		break;
-	case 4:
-	case 7:
-		memcpy(bus.bustype, "MCA   ", 6);
 	}
 	MP_bus_info(&bus);
 	if (mpc_default_type > 4) {
@@ -603,8 +597,8 @@ static int __init smp_scan_config(unsign
 	unsigned long mem;
 #endif
 
-	apic_printk(APIC_VERBOSE, "Scan SMP from %p for %ld bytes.\n",
-			bp, length);
+	apic_printk(APIC_VERBOSE, "Scan for SMP in [mem %#010lx-%#010lx]\n",
+		    base, base + length - 1);
 	BUILD_BUG_ON(sizeof(*mpf) != 16);
 
 	while (length > 0) {
@@ -620,16 +614,20 @@ static int __init smp_scan_config(unsign
 			mpf_found = mpf;
 
 #ifndef CONFIG_XEN
-			printk(KERN_INFO "found SMP MP-table at [%p] %llx\n",
-			       mpf, (u64)virt_to_phys(mpf));
+			printk(KERN_INFO "found SMP MP-table at [mem %#010llx-%#010llx] mapped at [%p]\n",
+			       (unsigned long long) virt_to_phys(mpf),
+			       (unsigned long long) virt_to_phys(mpf) +
+			       sizeof(*mpf) - 1, mpf);
 
 			mem = virt_to_phys(mpf);
 			memblock_reserve(mem, sizeof(*mpf));
 			if (mpf->physptr)
 				smp_reserve_memory(mpf);
 #else
-			printk(KERN_INFO "found SMP MP-table at [%p] %08lx\n",
-			       mpf, ((void *)bp - _bus_to_virt(base)) + base);
+			printk(KERN_INFO "found SMP MP-table at [mem %#010lx-%#010lx] mapped at [%p]\n",
+			       ((void *)bp - _bus_to_virt(base)) + base,
+			       ((void *)bp - _bus_to_virt(base)) + base +
+			       sizeof(*mpf) - 1, mpf);
 #endif
 			return 1;
 		}
@@ -659,7 +657,7 @@ void __init default_find_smp_config(void
 		return;
 	/*
 	 * If it is an SMP machine we should know now, unless the
-	 * configuration is in an EISA/MCA bus machine with an
+	 * configuration is in an EISA bus machine with an
 	 * extended bios data area.
 	 *
 	 * there is a real-mode segmented pointer pointing to the
--- head.orig/arch/x86/kernel/pci-dma-xen.c	2012-04-11 14:25:52.000000000 +0200
+++ head/arch/x86/kernel/pci-dma-xen.c	2012-06-19 12:14:10.000000000 +0200
@@ -131,6 +131,7 @@ void *dma_generic_alloc_coherent(struct 
 {
 	unsigned long dma_mask;
 	struct page *page;
+	unsigned int count = PAGE_ALIGN(size) >> PAGE_SHIFT;
 #ifndef CONFIG_XEN
 	dma_addr_t addr;
 #else
@@ -146,7 +147,11 @@ again:
 #else
 	flag &= ~(__GFP_DMA | __GFP_DMA32);
 #endif
-	page = alloc_pages_node(dev_to_node(dev), flag, order);
+	page = NULL;
+	if (!(flag & GFP_ATOMIC))
+		page = dma_alloc_from_contiguous(dev, count, order);
+	if (!page)
+		page = alloc_pages_node(dev_to_node(dev), flag, order);
 	if (!page)
 		return NULL;
 
@@ -178,17 +183,20 @@ again:
 #endif
 }
 
-#ifdef CONFIG_XEN
 void dma_generic_free_coherent(struct device *dev, size_t size, void *vaddr,
 			       dma_addr_t dma_addr, struct dma_attrs *attrs)
 {
-	unsigned int order = get_order(size);
-	unsigned long va = (unsigned long)vaddr;
+	unsigned int count = PAGE_ALIGN(size) >> PAGE_SHIFT;
+	struct page *page = virt_to_page(vaddr);
+
+	if (!dma_release_from_contiguous(dev, page, count)) {
+		unsigned int order = get_order(size);
+		unsigned long va = (unsigned long)vaddr;
 
-	xen_destroy_contiguous_region(va, order);
-	free_pages(va, order);
+		xen_destroy_contiguous_region(va, order);
+		free_pages(va, order);
+	}
 }
-#endif
 
 /*
  * See <Documentation/x86/x86_64/boot-options.txt> for the iommu kernel
--- head.orig/arch/x86/kernel/process-xen.c	2012-04-11 14:42:34.000000000 +0200
+++ head/arch/x86/kernel/process-xen.c	2012-06-14 11:23:26.000000000 +0200
@@ -28,6 +28,17 @@
 #include <asm/nmi.h>
 #include <xen/evtchn.h>
 
+#ifndef CONFIG_X86_NO_TSS
+/*
+ * per-CPU TSS segments. Threads are completely 'soft' on Linux,
+ * no more per-task TSS's. The TSS size is kept cacheline-aligned
+ * so they are allowed to end up in the .data..cacheline_aligned
+ * section. Since TSS's are completely CPU-local, we want them
+ * on exact cacheline boundaries, to eliminate cacheline ping-pong.
+ */
+DEFINE_PER_CPU_SHARED_ALIGNED(struct tss_struct, init_tss) = INIT_TSS;
+#endif
+
 #ifdef CONFIG_X86_64
 static DEFINE_PER_CPU(unsigned char, is_idle);
 static ATOMIC_NOTIFIER_HEAD(idle_notifier);
@@ -48,10 +59,16 @@ EXPORT_SYMBOL_GPL(idle_notifier_unregist
 struct kmem_cache *task_xstate_cachep;
 EXPORT_SYMBOL_GPL(task_xstate_cachep);
 
+/*
+ * this gets called so that we can store lazy state into memory and copy the
+ * current task into the new thread.
+ */
 int arch_dup_task_struct(struct task_struct *dst, struct task_struct *src)
 {
 	int ret;
 
+	unlazy_fpu(src);
+
 	*dst = *src;
 	if (fpu_allocated(&src->thread.fpu)) {
 		memset(&dst->thread.fpu, 0, sizeof(dst->thread.fpu));
@@ -68,10 +85,9 @@ void free_thread_xstate(struct task_stru
 	fpu_free(&tsk->thread.fpu);
 }
 
-void free_thread_info(struct thread_info *ti)
+void arch_release_task_struct(struct task_struct *tsk)
 {
-	free_thread_xstate(ti->task);
-	free_pages((unsigned long)ti, THREAD_ORDER);
+	free_thread_xstate(tsk);
 }
 
 void arch_task_cache_init(void)
@@ -82,6 +98,16 @@ void arch_task_cache_init(void)
 				  SLAB_PANIC | SLAB_NOTRACK, NULL);
 }
 
+static inline void drop_fpu(struct task_struct *tsk)
+{
+	/*
+	 * Forget coprocessor state..
+	 */
+	tsk->fpu_counter = 0;
+	clear_fpu(tsk);
+	clear_used_math();
+}
+
 /*
  * Free current thread data structures etc..
  */
@@ -105,12 +131,8 @@ void exit_thread(void)
 		t->io_bitmap_max = 0;
 		kfree(bp);
 	}
-}
 
-void show_regs(struct pt_regs *regs)
-{
-	show_registers(regs);
-	show_trace(NULL, regs, (unsigned long *)kernel_stack_pointer(regs), 0);
+	drop_fpu(me);
 }
 
 void show_regs_common(void)
@@ -145,12 +167,7 @@ void flush_thread(void)
 
 	flush_ptrace_hw_breakpoint(tsk);
 	memset(tsk->thread.tls_array, 0, sizeof(tsk->thread.tls_array));
-	/*
-	 * Forget coprocessor state..
-	 */
-	tsk->fpu_counter = 0;
-	clear_fpu(tsk);
-	clear_used_math();
+	drop_fpu(tsk);
 }
 
 static void hard_disable_TSC(void)
@@ -359,7 +376,7 @@ static inline void play_dead(void)
 #ifdef CONFIG_X86_64
 void enter_idle(void)
 {
-	percpu_write(is_idle, 1);
+	this_cpu_write(is_idle, 1);
 	atomic_notifier_call_chain(&idle_notifier, IDLE_START, NULL);
 }
 
@@ -493,26 +510,6 @@ void stop_this_cpu(void *dummy)
 	}
 }
 
-static void do_nothing(void *unused)
-{
-}
-
-/*
- * cpu_idle_wait - Used to ensure that all the CPUs discard old value of
- * pm_idle and update to new pm_idle value. Required while changing pm_idle
- * handler on SMP systems.
- *
- * Caller must have changed pm_idle to the new value before the call. Old
- * pm_idle value will not be used by any CPU after the return of this function.
- */
-void cpu_idle_wait(void)
-{
-	smp_mb();
-	/* kick all the CPUs so that they exit out of pm_idle */
-	smp_call_function(do_nothing, NULL, 1);
-}
-EXPORT_SYMBOL_GPL(cpu_idle_wait);
-
 #ifndef CONFIG_XEN
 /* Default MONITOR/MWAIT with no hints, used for default C1 state */
 static void mwait_idle(void)
@@ -574,9 +571,17 @@ int mwait_usable(const struct cpuinfo_x8
 {
 	u32 eax, ebx, ecx, edx;
 
+	/* Use mwait if idle=mwait boot option is given */
 	if (boot_option_idle_override == IDLE_FORCE_MWAIT)
 		return 1;
 
+	/*
+	 * Any idle= boot option other than idle=mwait means that we must not
+	 * use mwait. Eg: idle=halt or idle=poll or idle=nomwait
+	 */
+	if (boot_option_idle_override != IDLE_NO_OVERRIDE)
+		return 0;
+
 	if (c->cpuid_level < MWAIT_INFO)
 		return 0;
 
--- head.orig/arch/x86/kernel/process_32-xen.c	2012-08-01 12:13:13.000000000 +0200
+++ head/arch/x86/kernel/process_32-xen.c	2012-08-01 12:17:51.000000000 +0200
@@ -129,15 +129,6 @@ void release_thread(struct task_struct *
 	release_vm86_irqs(dead_task);
 }
 
-/*
- * This gets called before we allocate a new thread and copy
- * the current task into it.
- */
-void prepare_to_copy(struct task_struct *tsk)
-{
-	unlazy_fpu(tsk);
-}
-
 int copy_thread(unsigned long clone_flags, unsigned long sp,
 	unsigned long unused,
 	struct task_struct *p, struct pt_regs *regs)
@@ -327,7 +318,7 @@ __switch_to(struct task_struct *prev_p, 
 #endif
 	BUG_ON(mcl > _mcl + ARRAY_SIZE(_mcl));
 	if (_mcl->op == __HYPERVISOR_fpu_taskswitch) {
-		percpu_write(xen_x86_cr0_upd, X86_CR0_TS);
+		__this_cpu_write(xen_x86_cr0_upd, X86_CR0_TS);
 		cr0_ts = _mcl->args[0] ? 1 : -1;
 	} else
 		cr0_ts = 0;
@@ -335,9 +326,9 @@ __switch_to(struct task_struct *prev_p, 
 		BUG();
 	if (cr0_ts) {
 		if (cr0_ts > 0)
-			percpu_or(xen_x86_cr0, X86_CR0_TS);
+			__this_cpu_or(xen_x86_cr0, X86_CR0_TS);
 		else
-			percpu_and(xen_x86_cr0, ~X86_CR0_TS);
+			__this_cpu_and(xen_x86_cr0, ~X86_CR0_TS);
 		xen_clear_cr0_upd();
 	}
 
@@ -365,7 +356,7 @@ __switch_to(struct task_struct *prev_p, 
 
 	switch_fpu_finish(next_p, fpu);
 
-	percpu_write(current_task, next_p);
+	this_cpu_write(current_task, next_p);
 
 	return prev_p;
 }
--- head.orig/arch/x86/kernel/process_64-xen.c	2012-08-01 12:13:12.000000000 +0200
+++ head/arch/x86/kernel/process_64-xen.c	2012-08-01 12:17:53.000000000 +0200
@@ -154,15 +154,6 @@ static inline u32 read_32bit_tls(struct 
 	return get_desc_base(&t->thread.tls_array[tls]);
 }
 
-/*
- * This gets called before we allocate a new thread and copy
- * the current task into it.
- */
-void prepare_to_copy(struct task_struct *tsk)
-{
-	unlazy_fpu(tsk);
-}
-
 int copy_thread(unsigned long clone_flags, unsigned long sp,
 		unsigned long unused,
 	struct task_struct *p, struct pt_regs *regs)
@@ -365,7 +356,7 @@ __switch_to(struct task_struct *prev_p, 
 #endif
 	BUG_ON(mcl > _mcl + ARRAY_SIZE(_mcl));
 	if (_mcl->op == __HYPERVISOR_fpu_taskswitch) {
-		percpu_write(xen_x86_cr0_upd, X86_CR0_TS);
+		__this_cpu_write(xen_x86_cr0_upd, X86_CR0_TS);
 		cr0_ts = _mcl->args[0] ? 1 : -1;
 	} else
 		cr0_ts = 0;
@@ -373,9 +364,9 @@ __switch_to(struct task_struct *prev_p, 
 		BUG();
 	if (cr0_ts) {
 		if (cr0_ts > 0)
-			percpu_or(xen_x86_cr0, X86_CR0_TS);
+			__this_cpu_or(xen_x86_cr0, X86_CR0_TS);
 		else
-			percpu_and(xen_x86_cr0, ~X86_CR0_TS);
+			__this_cpu_and(xen_x86_cr0, ~X86_CR0_TS);
 		xen_clear_cr0_upd();
 	}
 
@@ -422,9 +413,9 @@ __switch_to(struct task_struct *prev_p, 
 	/*
 	 * Switch the PDA context.
 	 */
-	percpu_write(current_task, next_p);
+	this_cpu_write(current_task, next_p);
 
-	percpu_write(kernel_stack,
+	this_cpu_write(kernel_stack,
 		  (unsigned long)task_stack_page(next_p) +
 		  THREAD_SIZE - KERNEL_STACK_OFFSET);
 
--- head.orig/arch/x86/kernel/setup-xen.c	2012-04-11 13:26:23.000000000 +0200
+++ head/arch/x86/kernel/setup-xen.c	2012-06-14 12:01:06.000000000 +0200
@@ -34,7 +34,6 @@
 #include <linux/memblock.h>
 #include <linux/seq_file.h>
 #include <linux/console.h>
-#include <linux/mca.h>
 #include <linux/root_dev.h>
 #include <linux/highmem.h>
 #include <linux/module.h>
@@ -50,6 +49,7 @@
 #include <asm/pci-direct.h>
 #include <linux/init_ohci1394_dma.h>
 #include <linux/kvm_para.h>
+#include <linux/dma-contiguous.h>
 
 #include <linux/errno.h>
 #include <linux/kernel.h>
@@ -73,7 +73,6 @@
 
 #include <asm/mtrr.h>
 #include <asm/apic.h>
-#include <asm/trampoline.h>
 #include <asm/e820.h>
 #include <asm/mpspec.h>
 #include <asm/setup.h>
@@ -211,14 +210,8 @@ struct cpuinfo_x86 new_cpu_data __cpuini
 /* common cpu data for all cpus */
 struct cpuinfo_x86 boot_cpu_data __read_mostly = { .wp_works_ok = 1, .hard_math = 1 };
 EXPORT_SYMBOL(boot_cpu_data);
-#ifndef CONFIG_XEN
-static void set_mca_bus(int x)
-{
-#ifdef CONFIG_MCA
-	MCA_bus = x;
-#endif
-}
 
+#ifndef CONFIG_XEN
 unsigned int def_to_bigsmp;
 
 /* for MCA, but anyone else can use it if they want */
@@ -376,8 +369,8 @@ static void __init relocate_initrd(void)
 	memblock_reserve(ramdisk_here, area_size);
 	initrd_start = ramdisk_here + PAGE_OFFSET;
 	initrd_end   = initrd_start + ramdisk_size;
-	printk(KERN_INFO "Allocated new RAMDISK: %08llx - %08llx\n",
-			 ramdisk_here, ramdisk_here + ramdisk_size);
+	printk(KERN_INFO "Allocated new RAMDISK: [mem %#010llx-%#010llx]\n",
+			 ramdisk_here, ramdisk_here + ramdisk_size - 1);
 
 	q = (char *)initrd_start;
 
@@ -408,8 +401,8 @@ static void __init relocate_initrd(void)
 	/* high pages is not converted by early_res_to_bootmem */
 	ramdisk_image = boot_params.hdr.ramdisk_image;
 	ramdisk_size  = boot_params.hdr.ramdisk_size;
-	printk(KERN_INFO "Move RAMDISK from %016llx - %016llx to"
-		" %08llx - %08llx\n",
+	printk(KERN_INFO "Move RAMDISK from [mem %#010llx-%#010llx] to"
+		" [mem %#010llx-%#010llx]\n",
 		ramdisk_image, ramdisk_image + ramdisk_size - 1,
 		ramdisk_here, ramdisk_here + ramdisk_size - 1);
 #else
@@ -446,14 +439,13 @@ static void __init reserve_initrd(void)
 	initrd_start = 0;
 
 	if (ramdisk_size >= (end_of_lowmem>>1)) {
-		memblock_free(ramdisk_image, ramdisk_end - ramdisk_image);
-		printk(KERN_ERR "initrd too large to handle, "
-		       "disabling initrd\n");
-		return;
+		panic("initrd too large to handle, "
+		       "disabling initrd (%lu needed, %lu available)\n",
+		       ramdisk_size, end_of_lowmem>>1);
 	}
 
-	printk(KERN_INFO "RAMDISK: %08lx - %08lx\n", ramdisk_image,
-			ramdisk_end);
+	printk(KERN_INFO "RAMDISK: [mem %#010lx-%#010lx]\n", ramdisk_image,
+			ramdisk_end - 1);
 
 
 	if (ramdisk_end <= end_of_lowmem) {
@@ -810,7 +802,6 @@ void __init setup_arch(char **cmdline_p)
 	apm_info.bios = boot_params.apm_bios_info;
 	ist_info = boot_params.ist_info;
 	if (boot_params.sys_desc_table.length != 0) {
-		set_mca_bus(boot_params.sys_desc_table.table[3] & 0x2);
 		machine_id = boot_params.sys_desc_table.table[0];
 		machine_submodel_id = boot_params.sys_desc_table.table[1];
 		BIOS_revision = boot_params.sys_desc_table.table[2];
@@ -1039,11 +1030,11 @@ void __init setup_arch(char **cmdline_p)
 	setup_bios_corruption_check();
 #endif
 
-	printk(KERN_DEBUG "initial memory mapped : 0 - %08lx\n",
-			max_pfn_mapped<<PAGE_SHIFT);
+	printk(KERN_DEBUG "initial memory mapped: [mem 0x00000000-%#010lx]\n",
+			(max_pfn_mapped<<PAGE_SHIFT) - 1);
 
 #ifndef CONFIG_XEN
-	setup_trampolines();
+	setup_real_mode();
 #endif
 
 	init_gbpages();
@@ -1061,6 +1052,7 @@ void __init setup_arch(char **cmdline_p)
 	}
 #endif
 	memblock.current_limit = get_max_mapped();
+	dma_contiguous_reserve(0);
 
 	/*
 	 * NOTE: On x86-32, only from this point on, fixmaps are ready for use.
@@ -1111,6 +1103,8 @@ void __init setup_arch(char **cmdline_p)
 	if (boot_cpu_data.cpuid_level >= 0) {
 		/* A CPU has %cr4 if and only if it has CPUID */
 		mmu_cr4_features = read_cr4();
+		if (trampoline_cr4_features)
+			*trampoline_cr4_features = mmu_cr4_features;
 	}
 
 #if defined(CONFIG_X86_32) && !defined(CONFIG_XEN)
@@ -1228,7 +1222,8 @@ void __init setup_arch(char **cmdline_p)
 
 #ifndef CONFIG_XEN
 	init_apic_mappings();
-	ioapic_and_gsi_init();
+	if (x86_io_apic_ops.init)
+		x86_io_apic_ops.init();
 
 	kvm_guest_init();
 
--- head.orig/arch/x86/kernel/smp-xen.c	2012-02-16 17:53:11.000000000 +0100
+++ head/arch/x86/kernel/smp-xen.c	2012-06-14 11:54:35.000000000 +0200
@@ -109,6 +109,9 @@
  *	about nothing of note with C stepping upwards.
  */
 
+static atomic_t stopping_cpu = ATOMIC_INIT(-1);
+static bool smp_no_nmi_ipi = false;
+
 /*
  * this function sends a 'reschedule' IPI to another CPU.
  * it goes straight through and wastes no time serializing
@@ -133,9 +136,6 @@ void xen_send_call_func_ipi(const struct
 	xen_send_IPI_mask_allbutself(mask, CALL_FUNCTION_VECTOR);
 }
 
-static atomic_t stopping_cpu = ATOMIC_INIT(-1);
-static bool __read_mostly xen_smp_disable_nmi_ipi;
-
 static int smp_stop_nmi_callback(unsigned int val, struct pt_regs *regs)
 {
 	/* We are registered on stopping cpu too, avoid spurious NMI */
@@ -166,34 +166,26 @@ void xen_stop_other_cpus(int wait)
 	/*
 	 * Use an own vector here because smp_call_function
 	 * does lots of things not suitable in a panic situation.
-	 * On most systems we could also use an NMI here,
-	 * but there are a few systems around where NMI
-	 * is problematic so stay with an non NMI for now
-	 * (this implies we cannot stop CPUs spinning with irq off
-	 * currently)
 	 */
-	if (num_online_cpus() > 1) {
-		unsigned int vector = REBOOT_VECTOR;
 
-		if (!xen_smp_disable_nmi_ipi) {
-			/* did someone beat us here? */
-			if (atomic_cmpxchg(&stopping_cpu, -1,
-					   safe_smp_processor_id()) != -1)
-				return;
-
-			if (register_nmi_handler(NMI_LOCAL,
-						 smp_stop_nmi_callback,
-						 NMI_FLAG_FIRST, "smp_stop"))
-				/* Note: we ignore failures here */
-				return;
-
-			/* sync above data before sending NMI */
-			wmb();
+	/*
+	 * We start by using the REBOOT_VECTOR irq.
+	 * The irq is treated as a sync point to allow critical
+	 * regions of code on other cpus to release their spin locks
+	 * and re-enable irqs.  Jumping straight to an NMI might
+	 * accidentally cause deadlocks with further shutdown/panic
+	 * code.  By syncing, we give the cpus up to one second to
+	 * finish their work before we force them off with the NMI.
+	 */
+	if (num_online_cpus() > 1) {
+		/* did someone beat us here? */
+		if (atomic_cmpxchg(&stopping_cpu, -1, safe_smp_processor_id()) != -1)
+			return;
 
-			vector = NMI_VECTOR;
-		}
+		/* sync above data before sending IRQ */
+		wmb();
 
-		xen_send_IPI_allbutself(vector);
+		xen_send_IPI_allbutself(REBOOT_VECTOR);
 
 		/*
 		 * Don't wait longer than a second if the caller
@@ -204,6 +196,31 @@ void xen_stop_other_cpus(int wait)
 			udelay(1);
 	}
 
+	/* if the REBOOT_VECTOR didn't work, try with the NMI */
+	if ((num_online_cpus() > 1) && (!smp_no_nmi_ipi))  {
+		if (register_nmi_handler(NMI_LOCAL, smp_stop_nmi_callback,
+					 NMI_FLAG_FIRST, "smp_stop"))
+			/* Note: we ignore failures here */
+			/* Hope the REBOOT_IRQ is good enough */
+			goto finish;
+
+		/* sync above data before sending IRQ */
+		wmb();
+
+		pr_emerg("Shutting down cpus with NMI\n");
+
+		xen_send_IPI_allbutself(NMI_VECTOR);
+
+		/*
+		 * Don't wait longer than 10 ms if the caller
+		 * didn't ask us to wait.
+		 */
+		timeout = USEC_PER_MSEC * 10;
+		while (num_online_cpus() > 1 && (wait || timeout--))
+			udelay(1);
+	}
+
+finish:
 	local_irq_save(flags);
 	disable_all_local_evtchn();
 	local_irq_restore(flags);
@@ -237,8 +254,8 @@ irqreturn_t smp_call_function_single_int
 
 static int __init nonmi_ipi_setup(char *str)
 {
-        xen_smp_disable_nmi_ipi = true;
-        return 1;
+	smp_no_nmi_ipi = true;
+	return 1;
 }
 
 __setup("nonmi_ipi", nonmi_ipi_setup);
--- head.orig/arch/x86/kernel/traps-xen.c	2012-07-05 12:03:40.000000000 +0200
+++ head/arch/x86/kernel/traps-xen.c	2012-07-05 12:26:01.000000000 +0200
@@ -37,10 +37,6 @@
 #include <linux/eisa.h>
 #endif
 
-#ifdef CONFIG_MCA
-#include <linux/mca.h>
-#endif
-
 #if defined(CONFIG_EDAC)
 #include <linux/edac.h>
 #endif
@@ -50,6 +46,7 @@
 #include <asm/processor.h>
 #include <asm/debugreg.h>
 #include <linux/atomic.h>
+#include <asm/ftrace.h>
 #include <asm/traps.h>
 #include <asm/desc.h>
 #include <asm/i387.h>
@@ -307,8 +304,17 @@ gp_in_kernel:
 }
 
 /* May run on IST stack. */
-dotraplinkage void __kprobes do_int3(struct pt_regs *regs, long error_code)
+dotraplinkage void __kprobes notrace do_int3(struct pt_regs *regs, long error_code)
 {
+#ifdef CONFIG_DYNAMIC_FTRACE
+	/*
+	 * ftrace must be first, everything else may cause a recursive crash.
+	 * See note by declaration of modifying_ftrace_code in ftrace.c
+	 */
+	if (unlikely(atomic_read(&modifying_ftrace_code)) &&
+	    ftrace_int3_handler(regs))
+		return;
+#endif
 #ifdef CONFIG_KGDB_LOW_LEVEL_TRAP
 	if (kgdb_ll_trap(DIE_INT3, "int3", regs, error_code, X86_TRAP_BP,
 				SIGTRAP) == NOTIFY_STOP)
@@ -597,7 +603,7 @@ void math_state_restore(void)
 	struct task_struct *tsk = current;
 
 	/* NB. 'clts' is done for us by Xen during virtual trap. */
-	percpu_and(xen_x86_cr0, ~X86_CR0_TS);
+	__this_cpu_and(xen_x86_cr0, ~X86_CR0_TS);
 	if (!tsk_used_math(tsk)) {
 		local_irq_enable();
 		/*
--- head.orig/arch/x86/kernel/vsyscall_64-xen.c	2012-04-20 15:23:58.000000000 +0200
+++ head/arch/x86/kernel/vsyscall_64-xen.c	2012-07-25 09:34:59.000000000 +0200
@@ -141,6 +141,19 @@ static int addr_to_vsyscall_nr(unsigned 
 	return nr;
 }
 
+#ifdef CONFIG_SECCOMP
+static int vsyscall_seccomp(struct task_struct *tsk, int syscall_nr)
+{
+	if (!seccomp_mode(&tsk->seccomp))
+		return 0;
+	task_pt_regs(tsk)->orig_ax = syscall_nr;
+	task_pt_regs(tsk)->ax = syscall_nr;
+	return __secure_computing(syscall_nr);
+}
+#else
+#define vsyscall_seccomp(_tsk, _nr) 0
+#endif
+
 static bool write_ok_or_segv(unsigned long ptr, size_t size)
 {
 	/*
@@ -176,6 +189,7 @@ bool emulate_vsyscall(struct pt_regs *re
 	int vsyscall_nr;
 	int prev_sig_on_uaccess_error;
 	long ret;
+	int skip;
 
 	/*
 	 * No point in checking CS -- the only way to get here is a user mode
@@ -207,9 +221,6 @@ bool emulate_vsyscall(struct pt_regs *re
 	}
 
 	tsk = current;
-	if (seccomp_mode(&tsk->seccomp))
-		do_exit(SIGKILL);
-
 	/*
 	 * With a real vsyscall, page faults cause SIGSEGV.  We want to
 	 * preserve that behavior to make writing exploits harder.
@@ -224,8 +235,13 @@ bool emulate_vsyscall(struct pt_regs *re
 	 * address 0".
 	 */
 	ret = -EFAULT;
+	skip = 0;
 	switch (vsyscall_nr) {
 	case 0:
+		skip = vsyscall_seccomp(tsk, __NR_gettimeofday);
+		if (skip)
+			break;
+
 		if (!write_ok_or_segv(regs->di, sizeof(struct timeval)) ||
 		    !write_ok_or_segv(regs->si, sizeof(struct timezone)))
 			break;
@@ -236,6 +252,10 @@ bool emulate_vsyscall(struct pt_regs *re
 		break;
 
 	case 1:
+		skip = vsyscall_seccomp(tsk, __NR_time);
+		if (skip)
+			break;
+
 		if (!write_ok_or_segv(regs->di, sizeof(time_t)))
 			break;
 
@@ -243,6 +263,10 @@ bool emulate_vsyscall(struct pt_regs *re
 		break;
 
 	case 2:
+		skip = vsyscall_seccomp(tsk, __NR_getcpu);
+		if (skip)
+			break;
+
 		if (!write_ok_or_segv(regs->di, sizeof(unsigned)) ||
 		    !write_ok_or_segv(regs->si, sizeof(unsigned)))
 			break;
@@ -258,6 +282,12 @@ bool emulate_vsyscall(struct pt_regs *re
 
 	current_thread_info()->sig_on_uaccess_error = prev_sig_on_uaccess_error;
 
+	if (skip) {
+		if ((long)regs->ax <= 0L) /* seccomp errno emulation */
+			goto do_ret;
+		goto done; /* seccomp trace/trap */
+	}
+
 	if (ret == -EFAULT) {
 		/* Bad news -- userspace fed a bad pointer to a vsyscall. */
 		warn_bad_vsyscall(KERN_INFO, regs,
@@ -276,10 +306,11 @@ bool emulate_vsyscall(struct pt_regs *re
 
 	regs->ax = ret;
 
+do_ret:
 	/* Emulate a ret instruction. */
 	regs->ip = caller;
 	regs->sp += 8;
-
+done:
 	return true;
 
 sigsegv:
--- head.orig/arch/x86/mm/fault-xen.c	2012-04-11 13:26:23.000000000 +0200
+++ head/arch/x86/mm/fault-xen.c	2012-06-14 11:23:26.000000000 +0200
@@ -591,7 +591,7 @@ show_fault_oops(struct pt_regs *regs, un
 		pte_t *pte = lookup_address(address, &level);
 
 		if (pte && pte_present(*pte) && !pte_exec(*pte))
-			printk(nx_warning, current_uid());
+			printk(nx_warning, from_kuid(&init_user_ns, current_uid()));
 	}
 
 	printk(KERN_ALERT "BUG: unable to handle kernel ");
--- head.orig/arch/x86/mm/init-xen.c	2012-04-11 13:26:23.000000000 +0200
+++ head/arch/x86/mm/init-xen.c	2012-06-14 11:23:26.000000000 +0200
@@ -36,8 +36,14 @@ extern unsigned long extend_init_mapping
 extern void xen_finish_init_mapping(void);
 #endif
 
-static void __init find_early_table_space(unsigned long end, int use_pse,
-					  int use_gbpages)
+struct map_range {
+	unsigned long start;
+	unsigned long end;
+	unsigned page_size_mask;
+};
+
+static void __init find_early_table_space(struct map_range *mr, unsigned long end,
+					  int use_pse, int use_gbpages)
 {
 	unsigned long puds, pmds, ptes, tables;
 
@@ -61,6 +67,10 @@ static void __init find_early_table_spac
 #ifdef CONFIG_X86_32
 		extra += PMD_SIZE;
 #endif
+		/* The first 2/4M doesn't use large pages. */
+		if (mr->start < PMD_SIZE)
+			extra += mr->end - mr->start;
+
 		ptes = (extra + PAGE_SIZE - 1) >> PAGE_SHIFT;
 	} else
 		ptes = (end + PAGE_SIZE - 1) >> PAGE_SHIFT;
@@ -93,8 +103,9 @@ static void __init find_early_table_spac
 
 	pgt_buf_top = pgt_buf_start + (tables >> PAGE_SHIFT);
 
-	printk(KERN_DEBUG "kernel direct mapping tables up to %lx @ %lx-%lx\n",
-		end, pgt_buf_start << PAGE_SHIFT, pgt_buf_top << PAGE_SHIFT);
+	printk(KERN_DEBUG "kernel direct mapping tables up to %#lx @ [mem %#010lx-%#010lx]\n",
+		end - 1, pgt_buf_start << PAGE_SHIFT,
+		(pgt_buf_top << PAGE_SHIFT) - 1);
 }
 
 void __init xen_pagetable_reserve(u64 start, u64 end)
@@ -103,12 +114,6 @@ void __init xen_pagetable_reserve(u64 st
 		memblock_reserve(start, end - start);
 }
 
-struct map_range {
-	unsigned long start;
-	unsigned long end;
-	unsigned page_size_mask;
-};
-
 #ifdef CONFIG_X86_32
 #define NR_RANGE_MR 3
 #else /* CONFIG_X86_64 */
@@ -148,7 +153,8 @@ unsigned long __init_refok init_memory_m
 	int nr_range, i;
 	int use_pse, use_gbpages;
 
-	printk(KERN_INFO "init_memory_mapping: %016lx-%016lx\n", start, end);
+	printk(KERN_INFO "init_memory_mapping: [mem %#010lx-%#010lx]\n",
+	       start, end - 1);
 
 #if defined(CONFIG_DEBUG_PAGEALLOC) || defined(CONFIG_KMEMCHECK)
 	/*
@@ -267,8 +273,8 @@ unsigned long __init_refok init_memory_m
 	}
 
 	for (i = 0; i < nr_range; i++)
-		printk(KERN_DEBUG " %010lx - %010lx page %s\n",
-				mr[i].start, mr[i].end,
+		printk(KERN_DEBUG " [mem %#010lx-%#010lx] page %s\n",
+				mr[i].start, mr[i].end - 1,
 			(mr[i].page_size_mask & (1<<PG_LEVEL_1G))?"1G":(
 			 (mr[i].page_size_mask & (1<<PG_LEVEL_2M))?"2M":"4k"));
 
@@ -280,7 +286,7 @@ unsigned long __init_refok init_memory_m
 	 * nodes are discovered.
 	 */
 	if (!after_bootmem)
-		find_early_table_space(end, use_pse, use_gbpages);
+		find_early_table_space(&mr[0], end, use_pse, use_gbpages);
 
 #ifdef CONFIG_X86_64
 #define addr_to_page(addr)						\
@@ -409,8 +415,8 @@ void free_init_pages(char *what, unsigne
 	 * create a kernel page fault:
 	 */
 #ifdef CONFIG_DEBUG_PAGEALLOC
-	printk(KERN_INFO "debug: unmapping init memory %08lx..%08lx\n",
-		begin, end);
+	printk(KERN_INFO "debug: unmapping init [mem %#010lx-%#010lx]\n",
+		begin, end - 1);
 	set_memory_np(begin, (end - begin) >> PAGE_SHIFT);
 #else
 	/*
--- head.orig/arch/x86/mm/init_64-xen.c	2012-04-11 13:26:23.000000000 +0200
+++ head/arch/x86/mm/init_64-xen.c	2012-06-14 11:23:26.000000000 +0200
@@ -517,12 +517,12 @@ static unsigned long __meminit
 phys_pmd_init(pmd_t *pmd_page, unsigned long address, unsigned long end,
 	      unsigned long page_size_mask, pgprot_t prot)
 {
-	unsigned long pages = 0;
+	unsigned long pages = 0, next;
 	unsigned long last_map_addr = end;
 
 	int i = pmd_index(address);
 
-	for (; i < PTRS_PER_PMD; i++, address += PMD_SIZE) {
+	for (; i < PTRS_PER_PMD; i++, address = next) {
 		unsigned long pte_phys;
 		pmd_t *pmd = pmd_page + pmd_index(address);
 		pte_t *pte;
@@ -531,6 +531,8 @@ phys_pmd_init(pmd_t *pmd_page, unsigned 
 		if (address >= end)
 			break;
 
+		next = (address & PMD_MASK) + PMD_SIZE;
+
 		if (__pmd_val(*pmd)) {
 			if (!pmd_large(*pmd)) {
 				spin_lock(&init_mm.page_table_lock);
@@ -554,7 +556,7 @@ phys_pmd_init(pmd_t *pmd_page, unsigned 
 			 * attributes.
 			 */
 			if (page_size_mask & (1 << PG_LEVEL_2M)) {
-				pages++;
+				last_map_addr = next;
 				continue;
 			}
 			new_prot = pte_pgprot(pte_clrhuge(*(pte_t *)pmd));
@@ -567,7 +569,7 @@ phys_pmd_init(pmd_t *pmd_page, unsigned 
 				pfn_pte(address >> PAGE_SHIFT,
 					__pgprot(pgprot_val(prot) | _PAGE_PSE)));
 			spin_unlock(&init_mm.page_table_lock);
-			last_map_addr = (address & PMD_MASK) + PMD_SIZE;
+			last_map_addr = next;
 			continue;
 		}
 
@@ -604,11 +606,11 @@ static unsigned long __meminit
 phys_pud_init(pud_t *pud_page, unsigned long addr, unsigned long end,
 			 unsigned long page_size_mask)
 {
-	unsigned long pages = 0;
+	unsigned long pages = 0, next;
 	unsigned long last_map_addr = end;
 	int i = pud_index(addr);
 
-	for (; i < PTRS_PER_PUD; i++, addr = (addr & PUD_MASK) + PUD_SIZE) {
+	for (; i < PTRS_PER_PUD; i++, addr = next) {
 		unsigned long pmd_phys;
 		pud_t *pud = pud_page + pud_index(addr);
 		pmd_t *pmd;
@@ -617,6 +619,8 @@ phys_pud_init(pud_t *pud_page, unsigned 
 		if (addr >= end)
 			break;
 
+		next = (addr & PUD_MASK) + PUD_SIZE;
+
 		if (__pud_val(*pud)) {
 			if (!pud_large(*pud)) {
 				pmd = map_low_page(pmd_offset(pud, 0));
@@ -640,7 +644,7 @@ phys_pud_init(pud_t *pud_page, unsigned 
 			 * attributes.
 			 */
 			if (page_size_mask & (1 << PG_LEVEL_1G)) {
-				pages++;
+				last_map_addr = next;
 				continue;
 			}
 			prot = pte_pgprot(pte_clrhuge(*(pte_t *)pud));
@@ -652,7 +656,7 @@ phys_pud_init(pud_t *pud_page, unsigned 
 			set_pte((pte_t *)pud,
 				pfn_pte(addr >> PAGE_SHIFT, PAGE_KERNEL_LARGE));
 			spin_unlock(&init_mm.page_table_lock);
-			last_map_addr = (addr & PUD_MASK) + PUD_SIZE;
+			last_map_addr = next;
 			continue;
 		}
 
--- head.orig/arch/x86/mm/ioremap-xen.c	2011-07-01 15:19:35.000000000 +0200
+++ head/arch/x86/mm/ioremap-xen.c	2012-06-19 12:14:10.000000000 +0200
@@ -348,7 +348,7 @@ err_free_memtype:
 
 /**
  * ioremap_nocache     -   map bus memory into CPU space
- * @offset:    bus address of the memory
+ * @phys_addr:    bus address of the memory
  * @size:      size of the resource to map
  *
  * ioremap_nocache performs a platform specific sequence of operations to
@@ -385,7 +385,7 @@ EXPORT_SYMBOL(ioremap_nocache);
 
 /**
  * ioremap_wc	-	map memory into CPU space write combined
- * @offset:	bus address of the memory
+ * @phys_addr:	bus address of the memory
  * @size:	size of the resource to map
  *
  * This version of ioremap ensures that the memory is marked write combining.
--- head.orig/arch/x86/mm/pageattr-xen.c	2012-02-09 12:32:50.000000000 +0100
+++ head/arch/x86/mm/pageattr-xen.c	2012-06-19 12:14:10.000000000 +0200
@@ -122,7 +122,7 @@ within(unsigned long addr, unsigned long
 
 /**
  * clflush_cache_range - flush a cache range with clflush
- * @addr:	virtual start address
+ * @vaddr:	virtual start address
  * @size:	number of bytes to flush
  *
  * clflush is an unordered instruction which needs fencing with mfence
--- head.orig/arch/x86/mm/pat-xen.c	2011-02-01 15:03:10.000000000 +0100
+++ head/arch/x86/mm/pat-xen.c	2012-06-14 11:23:26.000000000 +0200
@@ -198,7 +198,7 @@ static int pat_pagerange_is_ram(resource
 		else
 			not_rampage = 1;
 
-		if (ram_page == not_rampage)
+		if (ram_page && not_rampage)
 			return -1;
 	}
 
@@ -230,9 +230,8 @@ static int reserve_ram_pages_type(u64 st
 		page = pfn_to_page(pfn);
 		type = get_page_memtype(page);
 		if (type != -1) {
-			printk(KERN_INFO "reserve_ram_pages_type failed "
-				"0x%Lx-0x%Lx, track 0x%lx, req 0x%lx\n",
-				start, end, type, req_type);
+			printk(KERN_INFO "reserve_ram_pages_type failed [mem %#010Lx-%#010Lx], track 0x%lx, req 0x%lx\n",
+				start, end - 1, type, req_type);
 			if (new_type)
 				*new_type = type;
 
@@ -338,9 +337,9 @@ int reserve_memtype(u64 start, u64 end, 
 
 	err = rbt_memtype_check_insert(new, new_type);
 	if (err) {
-		printk(KERN_INFO "reserve_memtype failed 0x%Lx-0x%Lx, "
-		       "track %s, req %s\n",
-		       start, end, cattr_name(new->type), cattr_name(req_type));
+		printk(KERN_INFO "reserve_memtype failed [mem %#010Lx-%#010Lx], track %s, req %s\n",
+		       start, end - 1,
+		       cattr_name(new->type), cattr_name(req_type));
 		kfree(new);
 		spin_unlock(&memtype_lock);
 
@@ -349,8 +348,8 @@ int reserve_memtype(u64 start, u64 end, 
 
 	spin_unlock(&memtype_lock);
 
-	dprintk("reserve_memtype added 0x%Lx-0x%Lx, track %s, req %s, ret %s\n",
-		start, end, cattr_name(new->type), cattr_name(req_type),
+	dprintk("reserve_memtype added [mem %#010Lx-%#010Lx], track %s, req %s, ret %s\n",
+		start, end - 1, cattr_name(new->type), cattr_name(req_type),
 		new_type ? cattr_name(*new_type) : "-");
 
 	return err;
@@ -384,14 +383,14 @@ int free_memtype(u64 start, u64 end)
 	spin_unlock(&memtype_lock);
 
 	if (!entry) {
-		printk(KERN_INFO "%s:%d freeing invalid memtype %Lx-%Lx\n",
-			current->comm, current->pid, start, end);
+		printk(KERN_INFO "%s:%d freeing invalid memtype [mem %#010Lx-%#010Lx]\n",
+		       current->comm, current->pid, start, end - 1);
 		return -EINVAL;
 	}
 
 	kfree(entry);
 
-	dprintk("free_memtype request 0x%Lx-0x%Lx\n", start, end);
+	dprintk("free_memtype request [mem %#010Lx-%#010Lx]\n", start, end - 1);
 
 	return 0;
 }
@@ -517,9 +516,8 @@ static inline int range_is_allowed(unsig
 
 	while (cursor < to) {
 		if (!devmem_is_allowed(mfn)) {
-			printk(KERN_INFO
-		"Program %s tried to access /dev/mem between %Lx->%Lx.\n",
-				current->comm, from, to);
+			printk(KERN_INFO "Program %s tried to access /dev/mem between [mem %#010Lx-%#010Lx]\n",
+				current->comm, from, to - 1);
 			return 0;
 		}
 		cursor += PAGE_SIZE;
@@ -602,12 +600,11 @@ static int reserve_pfn_range(u64 paddr, 
 
 		flags = lookup_memtype(paddr);
 		if (want_flags != flags) {
-			printk(KERN_WARNING
-			"%s:%d map pfn RAM range req %s for %Lx-%Lx, got %s\n",
+			printk(KERN_WARNING "%s:%d map pfn RAM range req %s for [mem %#010Lx-%#010Lx], got %s\n",
 				current->comm, current->pid,
 				cattr_name(want_flags),
 				(unsigned long long)paddr,
-				(unsigned long long)(paddr + size),
+				(unsigned long long)(paddr + size - 1),
 				cattr_name(flags));
 			*vma_prot = __pgprot((pgprot_val(*vma_prot) &
 					      (~_PAGE_CACHE_MASK)) |
@@ -625,11 +622,11 @@ static int reserve_pfn_range(u64 paddr, 
 		    !is_new_memtype_allowed(paddr, size, want_flags, flags)) {
 			free_memtype(paddr, paddr + size);
 			printk(KERN_ERR "%s:%d map pfn expected mapping type %s"
-				" for %Lx-%Lx, got %s\n",
+				" for [mem %#010Lx-%#010Lx], got %s\n",
 				current->comm, current->pid,
 				cattr_name(want_flags),
 				(unsigned long long)paddr,
-				(unsigned long long)(paddr + size),
+				(unsigned long long)(paddr + size - 1),
 				cattr_name(flags));
 			return -EINVAL;
 		}
--- head.orig/drivers/hwmon/coretemp-xen.c	2012-05-08 11:18:36.000000000 +0200
+++ head/drivers/hwmon/coretemp-xen.c	2012-07-05 12:46:12.000000000 +0200
@@ -199,6 +199,24 @@ static ssize_t show_temp(struct device *
 	return tdata->valid ? sprintf(buf, "%d\n", tdata->temp) : -EAGAIN;
 }
 
+struct tjmax {
+	char const *id;
+	int tjmax;
+};
+
+static const struct tjmax tjmax_table[] = {
+	{ "CPU D410", 100000 },
+	{ "CPU D425", 100000 },
+	{ "CPU D510", 100000 },
+	{ "CPU D525", 100000 },
+	{ "CPU N450", 100000 },
+	{ "CPU N455", 100000 },
+	{ "CPU N470", 100000 },
+	{ "CPU N475", 100000 },
+	{ "CPU  230", 100000 },
+	{ "CPU  330", 125000 },
+};
+
 static int adjust_tjmax(struct platform_data *c, u32 id, struct device *dev)
 {
 	/* The 100C is default for both mobile and non mobile CPUs */
@@ -209,6 +227,13 @@ static int adjust_tjmax(struct platform_
 	int err;
 	u32 eax, edx;
 	struct pci_dev *host_bridge;
+	int i;
+
+	/* explicit tjmax table entries override heuristics */
+	for (i = 0; i < ARRAY_SIZE(tjmax_table); i++) {
+		if (strstr(boot_cpu_data.x86_model_id, tjmax_table[i].id))
+			return tjmax_table[i].tjmax;
+	}
 
 	/* Early chips have no MSR for TjMax */
 
@@ -217,7 +242,8 @@ static int adjust_tjmax(struct platform_
 
 	/* Atom CPUs */
 
-	if (c->x86_model == 0x1c) {
+	if (c->x86_model == 0x1c || c->x86_model == 0x26
+	    || c->x86_model == 0x27) {
 		usemsr_ee = 0;
 
 		host_bridge = pci_get_bus_and_slot(0, PCI_DEVFN(0, 0));
@@ -230,6 +256,9 @@ static int adjust_tjmax(struct platform_
 			tjmax = 90000;
 
 		pci_dev_put(host_bridge);
+	} else if (c->x86_model == 0x36) {
+		usemsr_ee = 0;
+		tjmax = 100000;
 	}
 
 	if (c->x86_model > 0xe && usemsr_ee) {
@@ -825,17 +854,17 @@ static struct notifier_block coretemp_cp
 };
 
 static const struct x86_cpu_id coretemp_ids[] = {
-	{ X86_VENDOR_INTEL, X86_FAMILY_ANY, X86_MODEL_ANY, X86_FEATURE_DTS },
+	{ X86_VENDOR_INTEL, X86_FAMILY_ANY, X86_MODEL_ANY, X86_FEATURE_DTHERM },
 	{}
 };
 MODULE_DEVICE_TABLE(x86cpu, coretemp_ids);
 
 static int __init coretemp_init(void)
 {
-	int err = -ENODEV;
+	int err;
 
 	if (!is_initial_xendomain())
-		goto exit;
+		return -ENODEV;
 
 	/*
 	 * CPUID.06H.EAX[0] indicates whether the CPU has thermal
--- head.orig/drivers/xen/Makefile	2012-04-11 13:26:23.000000000 +0200
+++ head/drivers/xen/Makefile	2012-06-14 11:23:26.000000000 +0200
@@ -37,7 +37,7 @@ obj-$(CONFIG_XEN_SYS_HYPERVISOR)	+= sys-
 obj-$(CONFIG_XEN_PVHVM)			+= platform-pci.o
 obj-$(CONFIG_XEN_TMEM)			+= tmem.o
 obj-$(CONFIG_SWIOTLB_XEN)		+= swiotlb-xen.o
-obj-$(CONFIG_XEN_DOM0)			+= pci.o
+obj-$(CONFIG_XEN_DOM0)			+= pci.o acpi.o
 obj-$(CONFIG_XEN_PCIDEV_BACKEND)	+= xen-pciback/
 obj-$(CONFIG_XEN_PRIVCMD)		+= $(xen-privcmd_y)
 obj-$(CONFIG_XEN_ACPI_PROCESSOR)	+= xen-acpi-processor.o
--- head.orig/drivers/xen/core/evtchn.c	2012-04-12 09:56:01.000000000 +0200
+++ head/drivers/xen/core/evtchn.c	2012-06-14 12:21:59.000000000 +0200
@@ -191,7 +191,7 @@ static inline unsigned long active_evtch
 	shared_info_t *sh = HYPERVISOR_shared_info;
 
 	return (sh->evtchn_pending[idx] &
-		percpu_read(cpu_evtchn_mask[idx]) &
+		this_cpu_read(cpu_evtchn_mask[idx]) &
 		~sh->evtchn_mask[idx]);
 }
 
@@ -327,7 +327,7 @@ asmlinkage void __irq_entry evtchn_do_up
 		 * hardirq handlers see an up-to-date system time even if we
 		 * have just woken from a long idle period.
 		 */
-		if ((irq = percpu_read(virq_to_irq[VIRQ_TIMER])) != -1) {
+		if ((irq = __this_cpu_read(virq_to_irq[VIRQ_TIMER])) != -1) {
 			port = evtchn_from_irq(irq);
 			l1i = port / BITS_PER_LONG;
 			l2i = port % BITS_PER_LONG;
@@ -341,8 +341,8 @@ asmlinkage void __irq_entry evtchn_do_up
 
 		l1 = xchg(&vcpu_info->evtchn_pending_sel, 0);
 
-		start_l1i = l1i = percpu_read(current_l1i);
-		start_l2i = percpu_read(current_l2i);
+		start_l1i = l1i = __this_cpu_read(current_l1i);
+		start_l2i = __this_cpu_read(current_l2i);
 
 		for (i = 0; l1 != 0; i++) {
 			masked_l1 = l1 & ((~0UL) << l1i);
@@ -388,9 +388,9 @@ asmlinkage void __irq_entry evtchn_do_up
 				l2i = (l2i + 1) % BITS_PER_LONG;
 
 				/* Next caller starts at last processed + 1 */
-				percpu_write(current_l1i,
+				__this_cpu_write(current_l1i,
 					l2i ? l1i : (l1i + 1) % BITS_PER_LONG);
-				percpu_write(current_l2i, l2i);
+				__this_cpu_write(current_l2i, l2i);
 
 			} while (l2i != 0);
 
--- head.orig/drivers/xen/core/firmware.c	2012-06-12 15:36:15.000000000 +0200
+++ head/drivers/xen/core/firmware.c	2012-06-15 14:18:04.000000000 +0200
@@ -74,3 +74,31 @@ void __init copy_edid(void)
 		memset(edid_info.dummy, 0x13, sizeof(edid_info.dummy));
 #endif
 }
+
+#if defined(CONFIG_VT) && defined(CONFIG_X86)
+#include <asm/kbdleds.h>
+
+int __init kbd_defleds(void)
+{
+	int ret = 0;
+#if 0//todo
+	struct xen_platform_op op;
+
+	if (!is_initial_xendomain())
+		return 0;
+
+	op.cmd = XENPF_firmware_info;
+	op.u.firmware_info.index = 0;
+	op.u.firmware_info.type = XEN_FW_KBD_SHIFT_FLAGS;
+	if (HYPERVISOR_platform_op(&op) != 0)
+		return 0;
+	if (op.u.firmware_info.u.kbd_shift_flags & 0x10)
+		ret |= 1 << VC_SCROLLOCK;
+	if (op.u.firmware_info.u.kbd_shift_flags & 0x20)
+		ret |= 1 << VC_NUMLOCK;
+	if (op.u.firmware_info.u.kbd_shift_flags & 0x40)
+		ret |= 1 << VC_CAPSLOCK;
+#endif
+	return ret;
+}
+#endif
--- head.orig/drivers/xen/core/smpboot.c	2012-03-22 16:22:50.000000000 +0100
+++ head/drivers/xen/core/smpboot.c	2012-06-14 12:41:08.000000000 +0200
@@ -202,15 +202,14 @@ static void __cpuinit cpu_bringup_and_id
 	cpu_idle();
 }
 
-static void __cpuinit cpu_initialize_context(unsigned int cpu)
+static void __cpuinit cpu_initialize_context(unsigned int cpu,
+					     unsigned long sp0)
 {
 	/* vcpu_guest_context_t is too large to allocate on the stack.
 	 * Hence we allocate statically and protect it with a lock */
 	static vcpu_guest_context_t ctxt;
 	static DEFINE_SPINLOCK(ctxt_lock);
 
-	struct task_struct *idle = idle_task(cpu);
-
 	if (cpumask_test_and_set_cpu(cpu, vcpu_initialized_mask))
 		return;
 
@@ -231,10 +230,10 @@ static void __cpuinit cpu_initialize_con
 	ctxt.gdt_ents = GDT_SIZE / 8;
 
 	ctxt.user_regs.cs = __KERNEL_CS;
-	ctxt.user_regs.esp = idle->thread.sp0 - sizeof(struct pt_regs);
+	ctxt.user_regs.esp = sp0 - sizeof(struct pt_regs);
 
 	ctxt.kernel_ss = __KERNEL_DS;
-	ctxt.kernel_sp = idle->thread.sp0;
+	ctxt.kernel_sp = sp0;
 
 	ctxt.event_callback_eip    = (unsigned long)hypervisor_callback;
 	ctxt.failsafe_callback_eip = (unsigned long)failsafe_callback;
@@ -263,7 +262,6 @@ static void __cpuinit cpu_initialize_con
 void __init smp_prepare_cpus(unsigned int max_cpus)
 {
 	unsigned int cpu;
-	struct task_struct *idle;
 	int apicid;
 	struct vcpu_get_physid cpu_id;
 	void *gdt_addr;
@@ -292,10 +290,6 @@ void __init smp_prepare_cpus(unsigned in
 		if (cpu == 0)
 			continue;
 
-		idle = fork_idle(cpu);
-		if (IS_ERR(idle))
-			panic("failed fork for CPU %d", cpu);
-
 		gdt_addr = get_cpu_gdt_table(cpu);
 		make_page_readonly(gdt_addr, XENFEAT_writable_descriptor_tables);
 
@@ -305,14 +299,6 @@ void __init smp_prepare_cpus(unsigned in
 		cpu_data(cpu) = boot_cpu_data;
 		cpu_data(cpu).cpu_index = cpu;
 
-#ifdef __x86_64__
-		clear_tsk_thread_flag(idle, TIF_FORK);
-		per_cpu(kernel_stack, cpu) =
-			(unsigned long)task_stack_page(idle) -
-			KERNEL_STACK_OFFSET + THREAD_SIZE;
-#endif
-	 	per_cpu(current_task, cpu) = idle;
-
 		irq_ctx_init(cpu);
 
 #ifdef CONFIG_HOTPLUG_CPU
@@ -385,7 +371,7 @@ void __cpuinit __cpu_die(unsigned int cp
 
 #endif /* CONFIG_HOTPLUG_CPU */
 
-int __cpuinit __cpu_up(unsigned int cpu)
+int __cpuinit __cpu_up(unsigned int cpu, struct task_struct *idle)
 {
 	int rc;
 
@@ -397,7 +383,14 @@ int __cpuinit __cpu_up(unsigned int cpu)
 	if (rc)
 		return rc;
 
-	cpu_initialize_context(cpu);
+#ifdef CONFIG_X86_64
+	clear_tsk_thread_flag(idle, TIF_FORK);
+	per_cpu(kernel_stack, cpu) = (unsigned long)task_stack_page(idle) -
+				     KERNEL_STACK_OFFSET + THREAD_SIZE;
+#endif
+ 	per_cpu(current_task, cpu) = idle;
+
+	cpu_initialize_context(cpu, idle->thread.sp0);
 
 	if (num_online_cpus() == 1)
 		alternatives_smp_switch(1);
--- head.orig/drivers/xen/core/spinlock.c	2012-07-26 09:42:00.000000000 +0200
+++ head/drivers/xen/core/spinlock.c	2012-07-26 14:14:12.000000000 +0200
@@ -86,13 +86,13 @@ void __cpuinit spinlock_resume(void)
 
 static inline void sequence(unsigned int bias)
 {
-	unsigned int rm_idx = percpu_read(rm_seq.idx);
+	unsigned int rm_idx = __this_cpu_read(rm_seq.idx);
 
 	smp_wmb();
-	percpu_write(rm_seq.idx, (rm_idx + bias) ^ (SEQ_REMOVE_BIAS / 2));
+	__this_cpu_write(rm_seq.idx, (rm_idx + bias) ^ (SEQ_REMOVE_BIAS / 2));
 	smp_mb();
 	rm_idx &= 1;
-	while (percpu_read(rm_seq.ctr[rm_idx].counter))
+	while (__this_cpu_read(rm_seq.ctr[rm_idx].counter))
 		cpu_relax();
 }
 
@@ -129,7 +129,7 @@ static __ticket_t spin_adjust(struct spi
 struct __raw_tickets xen_spin_adjust(const arch_spinlock_t *lock,
 				     struct __raw_tickets token)
 {
-	token.tail = spin_adjust(percpu_read(_spinning), lock, token.tail);
+	token.tail = spin_adjust(__this_cpu_read(_spinning), lock, token.tail);
 	token.head = ACCESS_ONCE(lock->tickets.head);
 	return token;
 }
@@ -157,10 +157,10 @@ static unsigned int ticket_get(arch_spin
 
 void xen_spin_irq_enter(void)
 {
-	struct spinning *spinning = percpu_read(_spinning);
+	struct spinning *spinning = __this_cpu_read(_spinning);
 	unsigned int cpu = raw_smp_processor_id();
 
-	percpu_inc(_irq_count);
+	__this_cpu_inc(_irq_count);
 	smp_mb();
 	for (; spinning; spinning = spinning->prev) {
 		arch_spinlock_t *lock = spinning->lock;
@@ -189,7 +189,7 @@ void xen_spin_irq_enter(void)
 
 void xen_spin_irq_exit(void)
 {
-	struct spinning *spinning = percpu_read(_spinning);
+	struct spinning *spinning = __this_cpu_read(_spinning);
 	/*
 	 * Despite its counterpart being first in xen_spin_irq_enter() (to make
 	 * xen_spin_kick() properly handle locks that get owned after their
@@ -197,7 +197,7 @@ void xen_spin_irq_exit(void)
 	 * We're guaranteed to see another invocation of xen_spin_irq_enter()
 	 * if any of the tickets need to be dropped again.
 	 */
-	unsigned int irq_count = this_cpu_dec_return(_irq_count);
+	unsigned int irq_count = __this_cpu_dec_return(_irq_count);
 
 	/*
 	 * Make sure all xen_spin_kick() instances which may still have seen
@@ -233,13 +233,13 @@ bool xen_spin_wait(arch_spinlock_t *lock
 
 	/* If kicker interrupt not initialized yet, just spin. */
 	if (unlikely(!cpu_online(raw_smp_processor_id()))
-	    || unlikely(!percpu_read(poll_evtchn)))
+	    || unlikely(!__this_cpu_read(poll_evtchn)))
 		return false;
 
 	/* announce we're spinning */
 	spinning.ticket = ptok->tail;
 	spinning.lock = lock;
-	spinning.prev = percpu_read(_spinning);
+	spinning.prev = __this_cpu_read(_spinning);
 #if CONFIG_XEN_SPINLOCK_ACQUIRE_NESTING
 	spinning.irq_count = UINT_MAX;
 	if (upcall_mask > flags) {
@@ -255,10 +255,10 @@ bool xen_spin_wait(arch_spinlock_t *lock
 	arch_local_irq_disable();
 #endif
 	smp_wmb();
-	percpu_write(_spinning, &spinning);
+	__this_cpu_write(_spinning, &spinning);
 
 	for (;;) {
-		clear_evtchn(percpu_read(poll_evtchn));
+		clear_evtchn(__this_cpu_read(poll_evtchn));
 
 		/*
 		 * Check again to make sure it didn't become free while
@@ -271,19 +271,19 @@ bool xen_spin_wait(arch_spinlock_t *lock
 			 * without rechecking the lock.
 			 */
 			if (spinning.prev)
-				set_evtchn(percpu_read(poll_evtchn));
+				set_evtchn(__this_cpu_read(poll_evtchn));
 			break;
 		}
 
 #if CONFIG_XEN_SPINLOCK_ACQUIRE_NESTING
 		if (upcall_mask > flags) {
-			spinning.irq_count = percpu_read(_irq_count);
+			spinning.irq_count = __this_cpu_read(_irq_count);
 			smp_wmb();
 			arch_local_irq_restore(flags);
 		}
 #endif
 
-		if (!test_evtchn(percpu_read(poll_evtchn)) &&
+		if (!test_evtchn(__this_cpu_read(poll_evtchn)) &&
 		    HYPERVISOR_poll_no_timeout(&__get_cpu_var(poll_evtchn), 1))
 			BUG();
 
@@ -293,7 +293,7 @@ bool xen_spin_wait(arch_spinlock_t *lock
 		spinning.irq_count = UINT_MAX;
 #endif
 
-		if (test_evtchn(percpu_read(poll_evtchn))) {
+		if (test_evtchn(__this_cpu_read(poll_evtchn))) {
 			inc_irq_stat(irq_lock_count);
 			break;
 		}
@@ -305,7 +305,7 @@ bool xen_spin_wait(arch_spinlock_t *lock
 	 */
 
 	/* announce we're done */
-	percpu_write(_spinning, spinning.prev);
+	__this_cpu_write(_spinning, spinning.prev);
 	if (!CONFIG_XEN_SPINLOCK_ACQUIRE_NESTING)
 		arch_local_irq_disable();
 	sequence(SEQ_REMOVE_BIAS);
--- head.orig/drivers/xen/netback/loopback.c	2012-06-15 14:49:40.000000000 +0200
+++ head/drivers/xen/netback/loopback.c	2012-06-15 14:44:40.000000000 +0200
@@ -46,6 +46,7 @@
  */
 
 #include <linux/module.h>
+#include <linux/highmem.h>
 #include <linux/netdevice.h>
 #include <linux/inetdevice.h>
 #include <linux/etherdevice.h>
@@ -54,7 +55,6 @@
 #include <net/dst.h>
 #include <net/xfrm.h>		/* secpath_reset() */
 #include <asm/hypervisor.h>	/* is_initial_xendomain() */
-#include <../net/core/kmap_skb.h> /* k{,un}map_skb_frag() */
 
 static int nloopbacks = -1;
 module_param(nloopbacks, int, 0);
@@ -119,11 +119,11 @@ static int skb_remove_foreign_references
 		if (unlikely(!page))
 			return 0;
 
-		vaddr = kmap_skb_frag(f);
+		vaddr = kmap_atomic(skb_frag_page(f));
 		memcpy(page_address(page) + f->page_offset,
 		       vaddr + f->page_offset,
 		       skb_frag_size(f));
-		kunmap_skb_frag(vaddr);
+		kunmap_atomic(vaddr);
 
 		__skb_frag_unref(f);
 		__skb_frag_set_page(f, page);
--- head.orig/drivers/xen/xenbus/xenbus_comms.c	2011-04-11 15:03:17.000000000 +0200
+++ head/drivers/xen/xenbus/xenbus_comms.c	2012-06-14 11:23:26.000000000 +0200
@@ -280,3 +280,11 @@ int xb_init_comms(void)
 
 	return 0;
 }
+
+#if !defined(CONFIG_XEN) && !defined(MODULE)
+void xb_deinit_comms(void)
+{
+	unbind_from_irqhandler(xenbus_irq, &xb_waitq);
+	xenbus_irq = 0;
+}
+#endif
--- head.orig/drivers/xen/xenbus/xenbus_comms.h	2012-02-09 12:32:50.000000000 +0100
+++ head/drivers/xen/xenbus/xenbus_comms.h	2012-06-14 11:23:26.000000000 +0200
@@ -35,6 +35,7 @@
 
 int xs_init(void);
 int xb_init_comms(void);
+void xb_deinit_comms(void);
 
 /* Low level routines. */
 int xb_write(const void *data, unsigned len);
--- head.orig/drivers/xen/xenbus/xenbus_dev_backend.c	2012-06-13 16:00:49.000000000 +0200
+++ head/drivers/xen/xenbus/xenbus_dev_backend.c	2012-06-14 11:58:03.000000000 +0200
@@ -9,12 +9,12 @@
 #include <xen/xen.h>
 #ifdef CONFIG_PARAVIRT_XEN
 #include <xen/page.h>
-#endif
-#include <xen/xenbus.h>
-#include <xen/xenbus_dev.h>
 #include <xen/grant_table.h>
 #include <xen/events.h>
 #include <asm/xen/hypervisor.h>
+#endif
+#include <xen/xenbus.h>
+#include <xen/xenbus_dev.h>
 
 #include "xenbus_comms.h"
 
@@ -30,6 +30,7 @@ static int xenbus_backend_open(struct in
 
 static long xenbus_alloc(domid_t domid)
 {
+#ifdef CONFIG_PARAVIRT_XEN
 	struct evtchn_alloc_unbound arg;
 	int err = -EEXIST;
 
@@ -70,6 +71,9 @@ static long xenbus_alloc(domid_t domid)
  out_err:
 	xs_suspend_cancel();
 	return err;
+#else
+	return -EOPNOTSUPP;
+#endif
 }
 
 static long xenbus_backend_ioctl(struct file *file, unsigned int cmd, unsigned long data)
